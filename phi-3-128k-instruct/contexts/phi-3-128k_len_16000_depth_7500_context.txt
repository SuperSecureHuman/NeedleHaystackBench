<s><s><s> 
The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.
  

Want to start a startup?  Get funded by
Y Combinator.




November 2005Does "Web 2.0" mean anything?  Till recently I thought it didn't,
but the truth turns out to be more complicated.  Originally, yes,
it was meaningless.  Now it seems to have acquired a meaning.  And
yet those who dislike the term are probably right, because if it
means what I think it does, we don't need it.I first heard the phrase "Web 2.0" in the name of the Web 2.0
conference in 2004.  At the time it was supposed to mean using "the
web as a platform," which I took to refer to web-based applications.
[1]So I was surprised at a conference this summer when Tim O'Reilly
led a session intended to figure out a definition of "Web 2.0."
Didn't it already mean using the web as a platform?  And if it
didn't already mean something, why did we need the phrase at all?OriginsTim says the phrase "Web 2.0" first
arose in "a brainstorming session between
O'Reilly and Medialive International." What is Medialive International?
"Producers of technology tradeshows and conferences," according to
their site.  So presumably that's what this brainstorming session
was about.  O'Reilly wanted to organize a conference about the web,
and they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a
new version of the web.  They just wanted to make the point
that the web mattered again.  It was a kind of semantic deficit
spending: they knew new things were coming, and the "2.0" referred
to whatever those might turn out to be.And they were right.  New things were coming.  But the new version
number led to some awkwardness in the short term.  In the process
of developing the pitch for the first conference, someone must have
decided they'd better take a stab at explaining what that "2.0"
referred to.  Whatever it meant, "the web as a platform" was at
least not too constricting.The story about "Web 2.0" meaning the web as a platform didn't live
much past the first conference.  By the second conference, what
"Web 2.0" seemed to mean was something about democracy.  At least,
it did when people wrote about it online.  The conference itself
didn't seem very grassroots.  It cost $2800, so the only people who
could afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article
about the conference in Wired News spoke of "throngs of
geeks."  When a friend of mine asked Ryan about this, it was news
to him.  He said he'd originally written something like "throngs
of VCs and biz dev guys" but had later shortened it just to "throngs,"
and that this must have in turn been expanded by the editors into
"throngs of geeks."  After all, a Web 2.0 conference would presumably
be full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a   
suit, a sight so alien I couldn't parse it at first.  I saw
him walk by and said to one of the O'Reilly people "that guy looks
just like Tim.""Oh, that's Tim.  He bought a suit."
I ran after him, and sure enough, it was.  He explained that he'd
just bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows
during the Bubble, full of prowling VCs looking for the next hot
startup.  There was that same odd atmosphere created by a large  
number of people determined not to miss out.  Miss out on what?
They didn't know.  Whatever was going to happen—whatever Web 2.0
turned out to be.I wouldn't quite call it "Bubble 2.0" just because VCs are eager
to invest again.  The Internet is a genuinely big deal.  The bust
was as much an overreaction as
the boom.  It's to be expected that once we started to pull out of
the bust, there would be a lot of growth in this area, just as there
was in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO
market is gone.  Venture investors
are driven by exit strategies.  The reason they were funding all  
those laughable startups during the late 90s was that they hoped
to sell them to gullible retail investors; they hoped to be laughing
all the way to the bank.  Now that route is closed.  Now the default
exit strategy is to get bought, and acquirers are less prone to
irrational exuberance than IPO investors.  The closest you'll get 
to Bubble valuations is Rupert Murdoch paying $580 million for   
Myspace.  That's only off by a factor of 10 or so.1. AjaxDoes "Web 2.0" mean anything more than the name of a conference
yet?  I don't like to admit it, but it's starting to.  When people
say "Web 2.0" now, I have some idea what they mean.  And the fact
that I both despise the phrase and understand it is the surest proof
that it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still
only just bear to use without scare quotes.  Basically, what "Ajax"
means is "Javascript now works."  And that in turn means that
web-based applications can now be made to work much more like desktop
ones.As you read this, a whole new generation
of software is being written to take advantage of Ajax.  There
hasn't been such a wave of new applications since microcomputers
first appeared.  Even Microsoft sees it, but it's too late for them
to do anything more than leak "internal"  
documents designed to give the impression they're on top of this
new trend.In fact the new generation of software is being written way too
fast for Microsoft even to channel it, let alone write their own
in house.  Their only hope now is to buy all the best Ajax startups
before Google does.  And even that's going to be hard, because
Google has as big a head start in buying microstartups as it did
in search a few years ago.  After all, Google Maps, the canonical
Ajax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference
turned out to be partially right: web-based applications are a big
component of Web 2.0.  But I'm convinced they got this right by 
accident.  The Ajax boom didn't start till early 2005, when Google
Maps appeared and the term "Ajax" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several
examples to prove that amateurs can   
surpass professionals, when they have the right kind of system to 
channel their efforts.  Wikipedia
may be the most famous.  Experts have given Wikipedia middling
reviews, but they miss the critical point: it's good enough.  And   
it's free, which means people actually read it.  On the web, articles
you have to pay for might as well not exist.  Even if you were    
willing to pay to read them yourself, you can't link to them.    
They're not part of the conversation.Another place democracy seems to win is in deciding what counts as
news.  I never look at any news site now except Reddit.
[2]
 I know if something major
happens, or someone writes a particularly interesting article, it   
will show up there.  Why bother checking the front page of any
specific paper or magazine?  Reddit's like an RSS feed for the whole
web, with a filter for quality.  Similar sites include Digg, a technology news site that's
rapidly approaching Slashdot in popularity, and del.icio.us, the collaborative
bookmarking network that set off the "tagging" movement.  And whereas
Wikipedia's main appeal is that it's good enough and free, these
sites suggest that voters do a significantly better job than human
editors.The most dramatic example of Web 2.0 democracy is not in the selection
of ideas, but their production.  
I've noticed for a while that the stuff I read on individual people's
sites is as good as or better than the stuff I read in newspapers
and magazines.  And now I have independent evidence: the top links
on Reddit are generally links to individual people's sites rather  
than to magazine articles or news stories.My experience of writing
for magazines suggests an explanation.  Editors.  They control the
topics you can write about, and they can generally rewrite whatever
you produce.  The result is to damp extremes.  Editing yields 95th
percentile writing—95% of articles are improved by it, but 5% are
dragged down.  5% of the time you get "throngs of geeks."On the web, people can publish whatever they want.  Nearly all of
it falls short of the editor-damped writing in print publications.
But the pool of writers is very, very large.  If it's large enough,
the lack of damping means the best writing online should surpass  
the best in print.
[3]  
And now that the web has evolved mechanisms
for selecting good stuff, the web wins net.  Selection beats damping,
for the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the  
startups of the Bubble what bloggers are to the print media.  During
the Bubble, a startup meant a company headed by an MBA that was   
blowing through several million dollars of VC money to "get big
fast" in the most literal sense.  Now it means a smaller, younger, more technical group that just      
decided to make something great.  They'll decide later if they want  
to raise VC-scale funding, and if they take it, they'll take it on
their terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements
of "Web 2.0."  I also see a third: not to maltreat users.  During
the Bubble a lot of popular sites were quite high-handed with users.
And not just in obvious ways, like making them register, or subjecting
them to annoying ads.  The very design of the average site in the   
late 90s was an abuse.  Many of the most popular sites were loaded
with obtrusive branding that made them slow to load and sent the
user the message: this is our site, not yours.  (There's a physical
analog in the Intel and Microsoft stickers that come on some
laptops.)I think the root of the problem was that sites felt they were giving
something away for free, and till recently a company giving anything
away for free could be pretty high-handed about it.  Sometimes it
reached the point of economic sadism: site owners assumed that the
more pain they caused the user, the more benefit it must be to them.  
The most dramatic remnant of this model may be at salon.com, where   
you can read the beginning of a story, but to get the rest you have
sit through a movie.At Y Combinator we advise all the startups we fund never to lord
it over users.  Never make users register, unless you need to in
order to store something for them.  If you do make users register,   
never make them wait for a confirmation link in an email; in fact,
don't even ask for their email address unless you need it for some
reason.  Don't ask them any unnecessary questions.  Never send them
email unless they explicitly ask for it.  Never frame pages you
link to, or open them in new windows.  If you have a free version 
and a pay version, don't make the free version too restricted.  And
if you find yourself asking "should we allow users to do x?" just 
answer "yes" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups
never to let anyone fly under them, meaning never to let any other
company offer a cheaper, easier solution.  Another way to fly low 
is to give users more power.  Let users do what they want.  If you 
don't and a competitor does, you're in trouble.iTunes is Web 2.0ish in this sense.  Finally you can buy individual
songs instead of having to buy whole albums.  The recording industry
hated the idea and resisted it as long as possible.  But it was
obvious what users wanted, so Apple flew under the labels.
[4]
Though really it might be better to describe iTunes as Web 1.5.     
Web 2.0 applied to music would probably mean individual bands giving
away DRMless songs for free.The ultimate way to be nice to users is to give them something for
free that competitors charge for.  During the 90s a lot of people   
probably thought we'd have some working system for micropayments     
by now.  In fact things have gone in the other direction.  The most   
successful sites are the ones that figure out new ways to give stuff
away for free.  Craigslist has largely destroyed the classified ad
sites of the 90s, and OkCupid looks likely to do the same to the
previous generation of dating sites.Serving web pages is very, very cheap.  If you can make even a   
fraction of a cent per page view, you can make a profit.  And
technology for targeting ads continues to improve.  I wouldn't be
surprised if ten years from now eBay had been supplanted by an      
ad-supported freeBay (or, more likely, gBay).Odd as it might sound, we tell startups that they should try to
make as little money as possible.  If you can figure out a way to
turn a billion dollar industry into a fifty million dollar industry,
so much the better, if all fifty million go to you.  Though indeed,
making things cheaper often turns out to generate more money in the
end, just as automating things often turns out to generate more
jobs.The ultimate target is Microsoft.  What a bang that balloon is going
to make when someone pops it by offering a free web-based alternative 
to MS Office.
[5]
Who will?  Google?  They seem to be taking their
time.  I suspect the pin will be wielded by a couple of 20 year old
hackers who are too naive to be intimidated by the idea.  (How hard
can it be?)The Common ThreadAjax, democracy, and not dissing users.  What do they all have in  
common?  I didn't realize they had anything in common till recently,
which is one of the reasons I disliked the term "Web 2.0" so much.
It seemed that it was being used as a label for whatever happened
to be new—that it didn't predict anything.But there is a common thread.  Web 2.0 means using the web the way
it's meant to be used.  The "trends" we're seeing now are simply
the inherent nature of the web emerging from under the broken models
that got imposed on it during the Bubble.I realized this when I read an  interview with
Joe Kraus, the co-founder of Excite.
[6]

  Excite really never got the business model right at all.  We fell 
  into the classic problem of how when a new medium comes out it
  adopts the practices, the content, the business models of the old
  medium—which fails, and then the more appropriate models get
  figured out.

It may have seemed as if not much was happening during the years
after the Bubble burst.  But in retrospect, something was happening:
the web was finding its natural angle of repose.  The democracy 
component, for example—that's not an innovation, in the sense of
something someone made happen.  That's what the web naturally tends
to produce.Ditto for the idea of delivering desktop-like applications over the
web.  That idea is almost as old as the web.  But the first time    
around it was co-opted by Sun, and we got Java applets.  Java has
since been remade into a generic replacement for C++, but in 1996
the story about Java was that it represented a new model of software.
Instead of desktop applications, you'd run Java "applets" delivered
from a server.This plan collapsed under its own weight. Microsoft helped kill it,
but it would have died anyway.  There was no uptake among hackers.
When you find PR firms promoting
something as the next development platform, you can be sure it's
not.  If it were, you wouldn't need PR firms to tell you, because   
hackers would already be writing stuff on top of it, the way sites    
like Busmonster used Google Maps as a
platform before Google even meant it to be one.The proof that Ajax is the next hot platform is that thousands of  
hackers have spontaneously started building things on top
of it.  Mikey likes it.There's another thing all three components of Web 2.0 have in common.
Here's a clue.  Suppose you approached investors with the following
idea for a Web 2.0 startup:

  Sites like del.icio.us and flickr allow users to "tag" content
  with descriptive tokens.  But there is also huge source of
  implicit tags that they ignore: the text within web links.
  Moreover, these links represent a social network connecting the   
  individuals and organizations who created the pages, and by using
  graph theory we can compute from this network an estimate of the
  reputation of each member.  We plan to mine the web for these 
  implicit tags, and use them together with the reputation hierarchy
  they embody to enhance web searches.

How long do you think it would take them on average to realize that
it was a description of Google?Google was a pioneer in all three components of Web 2.0: their core
business sounds crushingly hip when described in Web 2.0 terms, 
"Don't maltreat users" is a subset of "Don't be evil," and of course
Google set off the whole Ajax boom with Google Maps.Web 2.0 means using the web as it was meant to be used, and Google
does.  That's their secret.    They're sailing with the wind, instead of sitting  
becalmed praying for a business model, like the print media, or   
trying to tack upwind by suing their customers, like Microsoft and 
the record labels.
[7]Google doesn't try to force things to happen their way.  They try   
to figure out what's going to happen, and arrange to be standing 
there when it does.  That's the way to approach technology—and 
as business includes an ever larger technological component, the
right way to do business.The fact that Google is a "Web 2.0" company shows that, while
meaningful, the term is also rather bogus.  It's like the word
"allopathic."  It just means doing things right, and it's a bad   
sign when you have a special word for that.
Notes[1]
From the conference
site, June 2004: "While the first wave of the Web was closely  
tied to the browser, the second wave extends applications across    
the web and enables a new generation of services and business
opportunities."  To the extent this means anything, it seems to be
about 
web-based applications.[2]
Disclosure: Reddit was funded by 
Y Combinator.  But although
I started using it out of loyalty to the home team, I've become a
genuine addict.  While we're at it, I'm also an investor in
!MSFT, having sold all my shares earlier this year.[3]
I'm not against editing. I spend more time editing than
writing, and I have a group of picky friends who proofread almost
everything I write.  What I dislike is editing done after the fact  
by someone else.[4]
Obvious is an understatement.  Users had been climbing in through  
the window for years before Apple finally moved the door.[5]
Hint: the way to create a web-based alternative to Office may
not be to write every component yourself, but to establish a protocol
for web-based apps to share a virtual home directory spread across
multiple servers.  Or it may be to write it all yourself.[6]
In Jessica Livingston's
Founders at
Work.[7]
Microsoft didn't sue their customers directly, but they seem 
to have done all they could to help SCO sue them.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, Peter
Norvig, Aaron Swartz, and Jeff Weiner for reading drafts of this, and to the
guys at O'Reilly and Adaptive Path for answering my questions.

Want to start a startup?  Get funded by
Y Combinator.




November 2009I don't think Apple realizes how badly the App Store approval process
is broken.  Or rather, I don't think they realize how much it matters
that it's broken.The way Apple runs the App Store has harmed their reputation with
programmers more than anything else they've ever done. 
Their reputation with programmers used to be great.
It used to be the most common complaint you heard
about Apple was that their fans admired them too uncritically.
The App Store has changed that.  Now a lot of programmers
have started to see Apple as evil.How much of the goodwill Apple once had with programmers have they
lost over the App Store?  A third?  Half?  And that's just so far.
The App Store is an ongoing karma leak.* * *How did Apple get into this mess?  Their fundamental problem is
that they don't understand software.They treat iPhone apps the way they treat the music they sell through
iTunes.  Apple is the channel; they own the user; if you want to
reach users, you do it on their terms. The record labels agreed,
reluctantly.  But this model doesn't work for software.  It doesn't
work for an intermediary to own the user.  The software business
learned that in the early 1980s, when companies like VisiCorp showed
that although the words "software" and "publisher" fit together,
the underlying concepts don't.  Software isn't like music or books.
It's too complicated for a third party to act as an intermediary
between developer and user.   And yet that's what Apple is trying
to be with the App Store: a software publisher.  And a particularly
overreaching one at that, with fussy tastes and a rigidly enforced
house style.If software publishing didn't work in 1980, it works even less now
that software development has evolved from a small number of big
releases to a constant stream of small ones.  But Apple doesn't
understand that either.  Their model of product development derives
from hardware.  They work on something till they think it's finished,
then they release it.  You have to do that with hardware, but because
software is so easy to change, its design can benefit from evolution.
The standard way to develop applications now is to launch fast and
iterate.  Which means it's a disaster to have long, random delays
each time you release a new version.Apparently Apple's attitude is that developers should be more careful
when they submit a new version to the App Store.  They would say
that.  But powerful as they are, they're not powerful enough to
turn back the evolution of technology.  Programmers don't use
launch-fast-and-iterate out of laziness.  They use it because it
yields the best results.  By obstructing that process, Apple is
making them do bad work, and programmers hate that as much as Apple
would.How would Apple like it if when they discovered a serious bug in
OS X, instead of releasing a software update immediately, they had
to submit their code to an intermediary who sat on it for a month
and then rejected it because it contained an icon they didn't like?By breaking software development, Apple gets the opposite of what
they intended: the version of an app currently available in the App
Store tends to be an old and buggy one.  One developer told me:

  As a result of their process, the App Store is full of half-baked
  applications. I make a new version almost every day that I release
  to beta users. The version on the App Store feels old and crappy.
  I'm sure that a lot of developers feel this way: One emotion is
  "I'm not really proud about what's in the App Store", and it's
  combined with the emotion "Really, it's Apple's fault."

Another wrote:

  I believe that they think their approval process helps users by
  ensuring quality.  In reality, bugs like ours get through all the
  time and then it can take 4-8 weeks to get that bug fix approved,
  leaving users to think that iPhone apps sometimes just don't work.
  Worse for Apple, these apps work just fine on other platforms
  that have immediate approval processes.

Actually I suppose Apple has a third misconception: that all the
complaints about App Store approvals are not a serious problem.
They must hear developers complaining.  But partners and suppliers
are always complaining.  It would be a bad sign if they weren't;
it would mean you were being too easy on them.  Meanwhile the iPhone
is selling better than ever.  So why do they need to fix anything?They get away with maltreating developers, in the short term, because
they make such great hardware.  I just bought a new 27" iMac a
couple days ago.  It's fabulous.  The screen's too shiny, and the
disk is surprisingly loud, but it's so beautiful that you can't
make yourself care.So I bought it, but I bought it, for the first time, with misgivings.
I felt the way I'd feel buying something made in a country with a
bad human rights record.  That was new.  In the past when I bought
things from Apple it was an unalloyed pleasure.  Oh boy!  They make
such great stuff.  This time it felt like a Faustian bargain.  They
make such great stuff, but they're such assholes.  Do I really want
to support this company?* * *Should Apple care what people like me think?  What difference does
it make if they alienate a small minority of their users?There are a couple reasons they should care.  One is that these
users are the people they want as employees.  If your company seems
evil, the best programmers won't work for you.  That hurt Microsoft
a lot starting in the 90s.  Programmers started to feel sheepish
about working there.  It seemed like selling out.  When people from
Microsoft were talking to other programmers and they mentioned where
they worked, there were a lot of self-deprecating jokes about having
gone over to the dark side.  But the real problem for Microsoft
wasn't the embarrassment of the people they hired.  It was the
people they never got.  And you know who got them?  Google and
Apple.  If Microsoft was the Empire, they were the Rebel Alliance.
And it's largely because they got more of the best people that
Google and Apple are doing so much better than Microsoft today.Why are programmers so fussy about their employers' morals?  Partly
because they can afford to be.  The best programmers can work
wherever they want.  They don't have to work for a company they
have qualms about.But the other reason programmers are fussy, I think, is that evil
begets stupidity.  An organization that wins by exercising power
starts to lose the ability to win by doing better work.  And it's
not fun for a smart person to work in a place where the best ideas
aren't the ones that win.  I think the reason Google embraced "Don't
be evil" so eagerly was not so much to impress the outside world
as to inoculate themselves against arrogance.
[1]That has worked for Google so far.  They've become more
bureaucratic, but otherwise they seem to have held true to their
original principles. With Apple that seems less the case.  When you
look at the famous 
1984 ad 
now, it's easier to imagine Apple as the
dictator on the screen than the woman with the hammer.
[2]
In fact, if you read the dictator's speech it sounds uncannily like a
prophecy of the App Store.

  We have triumphed over the unprincipled dissemination of facts.We have created, for the first time in all history, a garden of
  pure ideology, where each worker may bloom secure from the pests
  of contradictory and confusing truths.

The other reason Apple should care what programmers think of them
is that when you sell a platform, developers make or break you.  If
anyone should know this, Apple should.  VisiCalc made the Apple II.And programmers build applications for the platforms they use.  Most
applications—most startups, probably—grow out of personal projects.
Apple itself did.  Apple made microcomputers because that's what
Steve Wozniak wanted for himself.  He couldn't have afforded a
minicomputer. 
[3]
 Microsoft likewise started out making interpreters
for little microcomputers because
Bill Gates and Paul Allen were interested in using them.  It's a
rare startup that doesn't build something the founders use.The main reason there are so many iPhone apps is that so many programmers
have iPhones.  They may know, because they read it in an article,
that Blackberry has such and such market share.  But in practice
it's as if RIM didn't exist. If they're going to build something,
they want to be able to use it themselves, and that means building
an iPhone app.So programmers continue to develop iPhone apps, even though Apple
continues to maltreat them.  They're like someone stuck in an abusive
relationship.  They're so attracted to the iPhone that they can't
leave.  But they're looking for a way out.  One wrote:

  While I did enjoy developing for the iPhone, the control they
  place on the App Store does not give me the drive to develop
  applications as I would like. In fact I don't intend to make any
  more iPhone applications unless absolutely necessary.
[4]

Can anything break this cycle?  No device I've seen so far could.
Palm and RIM haven't a hope.  The only credible contender is Android.
But Android is an orphan; Google doesn't really care about it, not
the way Apple cares about the iPhone.  Apple cares about the iPhone
the way Google cares about search.* * *Is the future of handheld devices one locked down by Apple?  It's
a worrying prospect.  It would be a bummer to have another grim
monoculture like we had in the 1990s.  In 1995, writing software
for end users was effectively identical with writing Windows
applications.  Our horror at that prospect was the single biggest
thing that drove us to start building web apps.At least we know now what it would take to break Apple's lock.
You'd have to get iPhones out of programmers' hands.  If programmers
used some other device for mobile web access, they'd start to develop
apps for that instead.How could you make a device programmers liked better than the iPhone?
It's unlikely you could make something better designed.  Apple
leaves no room there.  So this alternative device probably couldn't
win on general appeal.  It would have to win by virtue of some
appeal it had to programmers specifically.One way to appeal to programmers is with software.  If you
could think of an application programmers had to have, but that
would be impossible in the circumscribed world of the iPhone, 
you could presumably get them to switch.That would definitely happen if programmers started to use handhelds
as development machines—if handhelds displaced laptops the
way laptops displaced desktops.  You need more control of a development
machine than Apple will let you have over an iPhone.Could anyone make a device that you'd carry around in your pocket
like a phone, and yet would also work as a development machine?
It's hard to imagine what it would look like.  But I've learned
never to say never about technology.  A phone-sized device that
would work as a development machine is no more miraculous by present
standards than the iPhone itself would have seemed by the standards
of 1995.My current development machine is a MacBook Air, which I use with
an external monitor and keyboard in my office, and by itself when
traveling.  If there was a version half the size I'd prefer it.
That still wouldn't be small enough to carry around everywhere like
a phone, but we're within a factor of 4 or so.  Surely that gap is
bridgeable.  In fact, let's make it an
RFS. Wanted: 
Woman with hammer.Notes[1]
When Google adopted "Don't be evil," they were still so small
that no one would have expected them to be, yet.
[2]
The dictator in the 1984 ad isn't Microsoft, incidentally;
it's IBM.  IBM seemed a lot more frightening in those days, but
they were friendlier to developers than Apple is now.[3]
He couldn't even afford a monitor.  That's why the Apple
I used a TV as a monitor.[4]
Several people I talked to mentioned how much they liked the
iPhone SDK.  The problem is not Apple's products but their policies.
Fortunately policies are software; Apple can change them instantly
if they want to.  Handy that, isn't it?Thanks to Sam Altman, Trevor Blackwell, Ross Boucher, 
James Bracy, Gabor Cselle,
Patrick Collison, Jason Freedman, John Gruber, Joe Hewitt, Jessica Livingston,
Robert Morris, Teng Siong Ong, Nikhil Pandit, Savraj Singh, and Jared Tame for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




July 2004(This essay is derived from a talk at Oscon 2004.)
A few months ago I finished a new 
book, 
and in reviews I keep
noticing words like "provocative'' and "controversial.'' To say
nothing of "idiotic.''I didn't mean to make the book controversial.  I was trying to make
it efficient.  I didn't want to waste people's time telling them
things they already knew.  It's more efficient just to give them
the diffs.  But I suppose that's bound to yield an alarming book.EdisonsThere's no controversy about which idea is most controversial:
the suggestion that variation in wealth might not be as big a
problem as we think.I didn't say in the book that variation in wealth was in itself a
good thing.  I said in some situations it might be a sign of good
things.  A throbbing headache is not a good thing, but it can be
a sign of a good thing-- for example, that you're recovering
consciousness after being hit on the head.Variation in wealth can be a sign of variation in productivity.
(In a society of one, they're identical.) And that
is almost certainly a good thing: if your society has no variation
in productivity, it's probably not because everyone is Thomas
Edison.  It's probably because you have no Thomas Edisons.In a low-tech society you don't see much variation in productivity.
If you have a tribe of nomads collecting sticks for a fire, how
much more productive is the best stick gatherer going to be than
the worst?  A factor of two?  Whereas when you hand people a complex tool
like a computer, the variation in what they can do with
it is enormous.That's not a new idea.  Fred Brooks wrote about it in 1974, and
the study he quoted was published in 1968.  But I think he
underestimated the variation between programmers.  He wrote about productivity in lines
of code:  the best programmers can solve a given problem in a tenth
the time.  But what if the problem isn't given? In programming, as
in many fields, the hard part isn't solving problems, but deciding
what problems to solve.  Imagination is hard to measure, but
in practice it dominates the kind of productivity that's measured
in lines of code.Productivity varies in any field, but there are few in which it
varies so much.  The variation between programmers
is so great that it becomes a difference in kind.  I don't
think this is something intrinsic to programming, though.  In every field,
technology magnifies differences in productivity.  I think what's
happening in programming is just that we have a lot of technological
leverage.  But in every field the lever is getting longer, so the
variation we see is something that more and more fields will see
as time goes on.  And the success of companies, and countries, will
depend increasingly on how they deal with it.If variation in productivity increases with technology, then the
contribution of the most productive individuals will not only be
disproportionately large, but will actually grow with time.  When
you reach the point where 90% of a group's output is created by 1%
of its members, you lose big if something (whether Viking raids,
or central planning) drags their productivity down to the average.If we want to get the most out of them, we need to understand these
especially productive people.  What motivates them?  What do they
need to do their jobs?  How do you recognize them? How do you
get them to come and work for you?  And then of course there's the
question, how do you become one?More than MoneyI know a handful of super-hackers, so I sat down and thought about
what they have in common.  Their defining quality is probably that
they really love to program.  Ordinary programmers write code to pay
the bills.  Great hackers think of it as something they do for fun,
and which they're delighted to find people will pay them for.Great programmers are sometimes said to be indifferent to money.
This isn't quite true.  It is true that all they really care about
is doing interesting work.  But if you make enough money, you get
to work on whatever you want, and for that reason hackers are
attracted by the idea of making really large amounts of money.
But as long as they still have to show up for work every day, they
care more about what they do there than how much they get paid for
it.Economically, this is a fact of the greatest importance, because
it means you don't have to pay great hackers anything like what
they're worth.  A great programmer might be ten or a hundred times
as productive as an ordinary one, but he'll consider himself lucky
to get paid three times as much.  As I'll explain later, this is
partly because great hackers don't know how good they are.  But
it's also because money is not the main thing they want.What do hackers want?  Like all craftsmen, hackers like good tools.
In fact, that's an understatement.  Good hackers find it unbearable
to use bad tools.  They'll simply refuse to work on projects with
the wrong infrastructure.At a startup I once worked for, one of the things pinned up on our
bulletin board was an ad from IBM.  It was a picture of an AS400,
and the headline read, I think, "hackers despise
it.'' [1]When you decide what infrastructure to use for a project, you're
not just making a technical decision.  You're also making a social
decision, and this may be the more important of the two.  For
example, if your company wants to write some software, it might
seem a prudent choice to write it in Java.  But when you choose a
language, you're also choosing a community.  The programmers you'll
be able to hire to work on a Java project won't be as
smart as the
ones you could get to work on a project written in Python.
And the quality of your hackers probably matters more than the
language you choose.  Though, frankly, the fact that good hackers
prefer Python to Java should tell you something about the relative
merits of those languages.Business types prefer the most popular languages because they view
languages as standards. They don't want to bet the company on
Betamax.  The thing about languages, though, is that they're not
just standards.  If you have to move bits over a network, by all
means use TCP/IP.  But a programming language isn't just a format.
A programming language is a medium of expression.I've read that Java has just overtaken Cobol as the most popular
language.  As a standard, you couldn't wish for more.  But as a
medium of expression, you could do a lot better.  Of all the great
programmers I can think of, I know of only one who would voluntarily
program in Java.  And of all the great programmers I can think of
who don't work for Sun, on Java, I know of zero.Great hackers also generally insist on using open source software.
Not just because it's better, but because it gives them more control.
Good hackers insist on control.  This is part of what makes them
good hackers:  when something's broken, they need to fix it.  You
want them to feel this way about the software they're writing for
you.  You shouldn't be surprised when they feel the same way about
the operating system.A couple years ago a venture capitalist friend told me about a new
startup he was involved with.  It sounded promising.  But the next
time I talked to him, he said they'd decided to build their software
on Windows NT, and had just hired a very experienced NT developer
to be their chief technical officer.  When I heard this, I thought,
these guys are doomed.  One, the CTO couldn't be a first rate
hacker, because to become an eminent NT developer he would have
had to use NT voluntarily, multiple times, and I couldn't imagine
a great hacker doing that; and two, even if he was good, he'd have
a hard time hiring anyone good to work for him if the project had
to be built on NT. [2]The Final FrontierAfter software, the most important tool to a hacker is probably
his office.  Big companies think the function of office space is to express
rank.  But hackers use their offices for more than that: they
use their office as a place to think in.  And if you're a technology
company, their thoughts are your product.  So making hackers work
in a noisy, distracting environment is like having a paint factory
where the air is full of soot.The cartoon strip Dilbert has a lot to say about cubicles, and with
good reason.  All the hackers I know despise them.  The mere prospect
of being interrupted is enough to prevent hackers from working on
hard problems.  If you want to get real work done in an office with
cubicles, you have two options: work at home, or come in early or
late or on a weekend, when no one else is there.  Don't companies
realize this is a sign that something is broken?  An office
environment is supposed to be something that helps
you work, not something you work despite.Companies like Cisco are proud that everyone there has a cubicle,
even the CEO.  But they're not so advanced as they think; obviously
they still view office space as a badge of rank.  Note too that
Cisco is famous for doing very little product development in house.
They get new technology by buying the startups that created it-- where
presumably the hackers did have somewhere quiet to work.One big company that understands what hackers need is Microsoft.
I once saw a recruiting ad for Microsoft with a big picture of a
door.  Work for us, the premise was, and we'll give you a place to
work where you can actually get work done.   And you know, Microsoft
is remarkable among big companies in that they are able to develop
software in house.  Not well, perhaps, but well enough.If companies want hackers to be productive, they should look at
what they do at home.  At home, hackers can arrange things themselves
so they can get the most done.  And when they work at home, hackers
don't work in noisy, open spaces; they work in rooms with doors.  They
work in cosy, neighborhoody places with people around and somewhere
to walk when they need to mull something over, instead of in glass
boxes set in acres of parking lots.  They have a sofa they can take
a nap on when they feel tired, instead of sitting in a coma at
their desk, pretending to work.  There's no crew of people with
vacuum cleaners that roars through every evening during the prime
hacking hours.  There are no meetings or, God forbid, corporate
retreats or team-building exercises.  And when you look at what
they're doing on that computer, you'll find it reinforces what I
said earlier about tools.  They may have to use Java and Windows
at work, but at home, where they can choose for themselves, you're
more likely to find them using Perl and Linux.Indeed, these statistics about Cobol or Java being the most popular
language can be misleading.  What we ought to look at, if we want
to know what tools are best, is what hackers choose when they can
choose freely-- that is, in projects of their own.  When you ask
that question, you find that open source operating systems already
have a dominant market share, and the number one language is probably
Perl.InterestingAlong with good tools, hackers want interesting projects.  What
makes a project interesting?  Well, obviously overtly sexy
applications like stealth planes or special effects software would
be interesting to work on.  But any application can be interesting
if it poses novel technical challenges.  So it's hard to predict
which problems hackers will like, because some become
interesting only when the people working on them discover a new
kind of solution.  Before ITA
(who wrote the software inside Orbitz),
the people working on airline fare searches probably thought it
was one of the most boring applications imaginable.  But ITA made
it interesting by 
redefining the problem in a more ambitious way.I think the same thing happened at Google.  When Google was founded,
the conventional wisdom among the so-called portals was that search
was boring and unimportant.  But the guys at Google didn't think
search was boring, and that's why they do it so well.This is an area where managers can make a difference.  Like a parent
saying to a child, I bet you can't clean up your whole room in
ten minutes, a good manager can sometimes redefine a problem as a
more interesting one.  Steve Jobs seems to be particularly good at
this, in part simply by having high standards.  There were a lot
of small, inexpensive computers before the Mac.  He redefined the
problem as: make one that's beautiful.  And that probably drove
the developers harder than any carrot or stick could.They certainly delivered.  When the Mac first appeared, you didn't
even have to turn it on to know it would be good; you could tell
from the case.  A few weeks ago I was walking along the street in
Cambridge, and in someone's trash I saw what appeared to be a Mac
carrying case.  I looked inside, and there was a Mac SE.  I carried
it home and plugged it in, and it booted.  The happy Macintosh
face, and then the finder.  My God, it was so simple.  It was just
like ... Google.Hackers like to work for people with high standards.  But it's not
enough just to be exacting.  You have to insist on the right things.
Which usually means that you have to be a hacker yourself.  I've
seen occasional articles about how to manage programmers.  Really
there should be two articles: one about what to do if
you are yourself a programmer, and one about what to do if you're not.  And the 
second could probably be condensed into two words:  give up.The problem is not so much the day to day management.  Really good
hackers are practically self-managing.  The problem is, if you're
not a hacker, you can't tell who the good hackers are.  A similar
problem explains why American cars are so ugly.  I call it the
design paradox.  You might think that you could make your products
beautiful just by hiring a great designer to design them.  But if
you yourself don't have good taste, 
how are you going to recognize
a good designer?  By definition you can't tell from his portfolio.
And you can't go by the awards he's won or the jobs he's had,
because in design, as in most fields, those tend to be driven by
fashion and schmoozing, with actual ability a distant third.
There's no way around it:  you can't manage a process intended to
produce beautiful things without knowing what beautiful is.  American
cars are ugly because American car companies are run by people with
bad taste.Many people in this country think of taste as something elusive,
or even frivolous.  It is neither.  To drive design, a manager must
be the most demanding user of a company's products.  And if you
have really good taste, you can, as Steve Jobs does, make satisfying
you the kind of problem that good people like to work on.Nasty Little ProblemsIt's pretty easy to say what kinds of problems are not interesting:
those where instead of solving a few big, clear, problems, you have
to solve a lot of nasty little ones.  One of the worst kinds of
projects is writing an interface to a piece of software that's
full of bugs.  Another is when you have to customize
something for an individual client's complex and ill-defined needs.
To hackers these kinds of projects are the death of a thousand
cuts.The distinguishing feature of nasty little problems is that you
don't learn anything from them.   Writing a compiler is interesting
because it teaches you what a compiler is.  But writing an interface
to a buggy piece of software doesn't teach you anything, because the
bugs are random.  [3] So it's not just fastidiousness that makes good
hackers avoid nasty little problems.  It's more a question of
self-preservation.  Working on nasty little problems makes you
stupid.  Good hackers avoid it for the same reason models avoid
cheeseburgers.Of course some problems inherently have this character.  And because
of supply and demand, they pay especially well.  So a company that
found a way to get great hackers to work on tedious problems would
be very successful.  How would you do it?One place this happens is in startups.  At our startup we had 
Robert Morris working as a system administrator.  That's like having the
Rolling Stones play at a bar mitzvah.  You can't hire that kind of
talent.  But people will do any amount of drudgery for companies
of which they're the founders.  [4]Bigger companies solve the problem by partitioning the company.
They get smart people to work for them by establishing a separate
R&D department where employees don't have to work directly on
customers' nasty little problems. [5] In this model, the research
department functions like a mine. They produce new ideas; maybe
the rest of the company will be able to use them.You may not have to go to this extreme.  
Bottom-up programming
suggests another way to partition the company: have the smart people
work as toolmakers.  If your company makes software to do x, have
one group that builds tools for writing software of that type, and
another that uses these tools to write the applications.  This way
you might be able to get smart people to write 99% of your code,
but still keep them almost as insulated from users as they would
be in a traditional research department.  The toolmakers would have
users, but they'd only be the company's own developers.  [6]If Microsoft used this approach, their software wouldn't be so full
of security holes, because the less smart people writing the actual
applications wouldn't be doing low-level stuff like allocating
memory.  Instead of writing Word directly in C, they'd be plugging
together big Lego blocks of Word-language.  (Duplo, I believe, is
the technical term.)ClumpingAlong with interesting problems, what good hackers like is other
good hackers.  Great hackers tend to clump together-- sometimes
spectacularly so, as at Xerox Parc.   So you won't attract good
hackers in linear proportion to how good an environment you create
for them.  The tendency to clump means it's more like the square
of the environment.  So it's winner take all.  At any given time,
there are only about ten or twenty places where hackers most want to
work, and if you aren't one of them, you won't just have fewer
great hackers, you'll have zero.Having great hackers is not, by itself, enough to make a company
successful.  It works well for Google and ITA, which are two of
the hot spots right now, but it didn't help Thinking Machines or
Xerox.  Sun had a good run for a while, but their business model
is a down elevator.  In that situation, even the best hackers can't
save you.I think, though, that all other things being equal, a company that
can attract great hackers will have a huge advantage.  There are
people who would disagree with this.  When we were making the rounds
of venture capital firms in the 1990s, several told us that software
companies didn't win by writing great software, but through brand,
and dominating channels, and doing the right deals.They really seemed to believe this, and I think I know why.  I
think what a lot of VCs are looking for, at least unconsciously,
is the next Microsoft.  And of course if Microsoft is your model,
you shouldn't be looking for companies that hope to win by writing
great software.  But VCs are mistaken to look for the next Microsoft,
because no startup can be the next Microsoft unless some other
company is prepared to bend over at just the right moment and be
the next IBM.It's a mistake to use Microsoft as a model, because their whole
culture derives from that one lucky break.  Microsoft is a bad data
point.  If you throw them out, you find that good products do tend
to win in the market.  What VCs should be looking for is the next
Apple, or the next Google.I think Bill Gates knows this.  What worries him about Google is
not the power of their brand, but the fact that they have
better hackers. [7]
RecognitionSo who are the great hackers?  How do you know when you meet one?
That turns out to be very hard.  Even hackers can't tell.  I'm
pretty sure now that my friend Trevor Blackwell is a great hacker.
You may have read on Slashdot how he made his 
own Segway.  The
remarkable thing about this project was that he wrote all the
software in one day (in Python, incidentally).For Trevor, that's
par for the course.  But when I first met him, I thought he was a
complete idiot.  He was standing in Robert Morris's office babbling
at him about something or other, and I remember standing behind
him making frantic gestures at Robert to shoo this nut out of his
office so we could go to lunch.  Robert says he misjudged Trevor
at first too.  Apparently when Robert first met him, Trevor had
just begun a new scheme that involved writing down everything about
every aspect of his life on a stack of index cards, which he carried
with him everywhere.  He'd also just arrived from Canada, and had
a strong Canadian accent and a mullet.The problem is compounded by the fact that hackers, despite their
reputation for social obliviousness, sometimes put a good deal of
effort into seeming smart.  When I was in grad school I used to
hang around the MIT AI Lab occasionally. It was kind of intimidating
at first.  Everyone there spoke so fast.  But after a while I
learned the trick of speaking fast.  You don't have to think any
faster; just use twice as many words to say everything.  With this amount of noise in the signal, it's hard to tell good
hackers when you meet them.  I can't tell, even now.  You also
can't tell from their resumes.  It seems like the only way to judge
a hacker is to work with him on something.And this is the reason that high-tech areas 
only happen around universities.  The active ingredient
here is not so much the professors as the students.  Startups grow up
around universities because universities bring together promising young
people and make them work on the same projects.  The
smart ones learn who the other smart ones are, and together
they cook up new projects of their own.Because you can't tell a great hacker except by working with him,
hackers themselves can't tell how good they are.  This is true to
a degree in most fields.  I've found that people who
are great at something are not so much convinced of their own
greatness as mystified at why everyone else seems so incompetent.
But it's particularly hard for hackers to know how good they are,
because it's hard to compare their work.  This is easier in most
other fields.  In the hundred meters, you know in 10 seconds who's
fastest.  Even in math there seems to be a general consensus about
which problems are hard to solve, and what constitutes a good
solution.  But hacking is like writing.  Who can say which of two
novels is better?  Certainly not the authors.With hackers, at least, other hackers can tell.  That's because,
unlike novelists, hackers collaborate on projects.  When you get
to hit a few difficult problems over the net at someone, you learn
pretty quickly how hard they hit them back.  But hackers can't
watch themselves at work.  So if you ask a great hacker how good
he is, he's almost certain to reply, I don't know.  He's not just
being modest.  He really doesn't know.And none of us know, except about people we've actually worked
with.  Which puts us in a weird situation: we don't know who our
heroes should be.  The hackers who become famous tend to become
famous by random accidents of PR.  Occasionally I need to give an
example of a great hacker, and I never know who to use.  The first
names that come to mind always tend to be people I know personally,
but it seems lame to use them.  So, I think, maybe I should say
Richard Stallman, or Linus Torvalds, or Alan Kay, or someone famous
like that.  But I have no idea if these guys are great hackers.
I've never worked with them on anything.If there is a Michael Jordan of hacking, no one knows, including
him.CultivationFinally, the question the hackers have all been wondering about:
how do you become a great hacker?  I don't know if it's possible
to make yourself into one.  But it's certainly possible to do things
that make you stupid, and if you can make yourself stupid, you
can probably make yourself smart too.The key to being a good hacker may be to work on what you like.
When I think about the great hackers I know, one thing they have
in common is the extreme 
difficulty of making them work 
on anything they
don't want to.  I don't know if this is cause or effect; it may be
both.To do something well you have to love it.  
So to the extent you
can preserve hacking as something you love, you're likely to do it
well.  Try to keep the sense of wonder you had about programming at
age 14.  If you're worried that your current job is rotting your
brain, it probably is.The best hackers tend to be smart, of course, but that's true in
a lot of fields.  Is there some quality that's unique to hackers?
I asked some friends, and the number one thing they mentioned was
curiosity.  
I'd always supposed that all smart people were curious--
that curiosity was simply the first derivative of knowledge.  But
apparently hackers are particularly curious, especially about how
things work.  That makes sense, because programs are in effect
giant descriptions of how things work.Several friends mentioned hackers' ability to concentrate-- their
ability, as one put it, to "tune out everything outside their own
heads.''  I've certainly noticed this.  And I've heard several 
hackers say that after drinking even half a beer they can't program at
all.   So maybe hacking does require some special ability to focus.
Perhaps great hackers can load a large amount of context into their
head, so that when they look at a line of code, they see not just
that line but the whole program around it.  John McPhee
wrote that Bill Bradley's success as a basketball player was due
partly to his extraordinary peripheral vision.  "Perfect'' eyesight
means about 47 degrees of vertical peripheral vision.  Bill Bradley
had 70; he could see the basket when he was looking at the floor.
Maybe great hackers have some similar inborn ability.  (I cheat by
using a very dense language, 
which shrinks the court.)This could explain the disconnect over cubicles.  Maybe the people
in charge of facilities, not having any concentration to shatter,
have no idea that working in a cubicle feels to a hacker like having
one's brain in a blender.  (Whereas Bill, if the rumors of autism
are true, knows all too well.)One difference I've noticed between great hackers and smart people
in general is that hackers are more 
politically incorrect.  To the
extent there is a secret handshake among good hackers, it's when they
know one another well enough to express opinions that would get
them stoned to death by the general public.  And I can see why
political incorrectness would be a useful quality in programming.
Programs are very complex and, at least in the hands of good
programmers, very fluid.  In such situations it's helpful to have
a habit of questioning assumptions.Can you cultivate these qualities?  I don't know.  But you can at
least not repress them.  So here is my best shot at a recipe.  If
it is possible to make yourself into a great hacker, the way to do
it may be to make the following deal with yourself: you never have
to work on boring projects (unless your family will starve otherwise),
and in return, you'll never allow yourself to do a half-assed job.
All the great hackers I know seem to have made that deal, though
perhaps none of them had any choice in the matter.Notes
[1] In fairness, I have to say that IBM makes decent hardware.  I
wrote this on an IBM laptop.[2] They did turn out to be doomed.  They shut down a few months
later.[3] I think this is what people mean when they talk
about the "meaning of life."  On the face of it, this seems an 
odd idea.  Life isn't an expression; how could it have meaning?
But it can have a quality that feels a lot like meaning.  In a project
like a compiler, you have to solve a lot of problems, but the problems
all fall into a pattern, as in a