<s><s>  

Want to start a startup?  Get funded by
Y Combinator.




November 2005Does "Web 2.0" mean anything?  Till recently I thought it didn't,
but the truth turns out to be more complicated.  Originally, yes,
it was meaningless.  Now it seems to have acquired a meaning.  And
yet those who dislike the term are probably right, because if it
means what I think it does, we don't need it.I first heard the phrase "Web 2.0" in the name of the Web 2.0
conference in 2004.  At the time it was supposed to mean using "the
web as a platform," which I took to refer to web-based applications.
[1]So I was surprised at a conference this summer when Tim O'Reilly
led a session intended to figure out a definition of "Web 2.0."
Didn't it already mean using the web as a platform?  And if it
didn't already mean something, why did we need the phrase at all?OriginsTim says the phrase "Web 2.0" first
arose in "a brainstorming session between
O'Reilly and Medialive International." What is Medialive International?
"Producers of technology tradeshows and conferences," according to
their site.  So presumably that's what this brainstorming session
was about.  O'Reilly wanted to organize a conference about the web,
and they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a
new version of the web.  They just wanted to make the point
that the web mattered again.  It was a kind of semantic deficit
spending: they knew new things were coming, and the "2.0" referred
to whatever those might turn out to be.And they were right.  New things were coming.  But the new version
number led to some awkwardness in the short term.  In the process
of developing the pitch for the first conference, someone must have
decided they'd better take a stab at explaining what that "2.0"
referred to.  Whatever it meant, "the web as a platform" was at
least not too constricting.The story about "Web 2.0" meaning the web as a platform didn't live
much past the first conference.  By the second conference, what
"Web 2.0" seemed to mean was something about democracy.  At least,
it did when people wrote about it online.  The conference itself
didn't seem very grassroots.  It cost $2800, so the only people who
could afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article
about the conference in Wired News spoke of "throngs of
geeks."  When a friend of mine asked Ryan about this, it was news
to him.  He said he'd originally written something like "throngs
of VCs and biz dev guys" but had later shortened it just to "throngs,"
and that this must have in turn been expanded by the editors into
"throngs of geeks."  After all, a Web 2.0 conference would presumably
be full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a   
suit, a sight so alien I couldn't parse it at first.  I saw
him walk by and said to one of the O'Reilly people "that guy looks
just like Tim.""Oh, that's Tim.  He bought a suit."
I ran after him, and sure enough, it was.  He explained that he'd
just bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows
during the Bubble, full of prowling VCs looking for the next hot
startup.  There was that same odd atmosphere created by a large  
number of people determined not to miss out.  Miss out on what?
They didn't know.  Whatever was going to happen—whatever Web 2.0
turned out to be.I wouldn't quite call it "Bubble 2.0" just because VCs are eager
to invest again.  The Internet is a genuinely big deal.  The bust
was as much an overreaction as
the boom.  It's to be expected that once we started to pull out of
the bust, there would be a lot of growth in this area, just as there
was in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO
market is gone.  Venture investors
are driven by exit strategies.  The reason they were funding all  
those laughable startups during the late 90s was that they hoped
to sell them to gullible retail investors; they hoped to be laughing
all the way to the bank.  Now that route is closed.  Now the default
exit strategy is to get bought, and acquirers are less prone to
irrational exuberance than IPO investors.  The closest you'll get 
to Bubble valuations is Rupert Murdoch paying $580 million for   
Myspace.  That's only off by a factor of 10 or so.1. AjaxDoes "Web 2.0" mean anything more than the name of a conference
yet?  I don't like to admit it, but it's starting to.  When people
say "Web 2.0" now, I have some idea what they mean.  And the fact
that I both despise the phrase and understand it is the surest proof
that it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still
only just bear to use without scare quotes.  Basically, what "Ajax"
means is "Javascript now works."  And that in turn means that
web-based applications can now be made to work much more like desktop
ones.As you read this, a whole new generation
of software is being written to take advantage of Ajax.  There
hasn't been such a wave of new applications since microcomputers
first appeared.  Even Microsoft sees it, but it's too late for them
to do anything more than leak "internal"  
documents designed to give the impression they're on top of this
new trend.In fact the new generation of software is being written way too
fast for Microsoft even to channel it, let alone write their own
in house.  Their only hope now is to buy all the best Ajax startups
before Google does.  And even that's going to be hard, because
Google has as big a head start in buying microstartups as it did
in search a few years ago.  After all, Google Maps, the canonical
Ajax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference
turned out to be partially right: web-based applications are a big
component of Web 2.0.  But I'm convinced they got this right by 
accident.  The Ajax boom didn't start till early 2005, when Google
Maps appeared and the term "Ajax" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several
examples to prove that amateurs can   
surpass professionals, when they have the right kind of system to 
channel their efforts.  Wikipedia
may be the most famous.  Experts have given Wikipedia middling
reviews, but they miss the critical point: it's good enough.  And   
it's free, which means people actually read it.  On the web, articles
you have to pay for might as well not exist.  Even if you were    
willing to pay to read them yourself, you can't link to them.    
They're not part of the conversation.Another place democracy seems to win is in deciding what counts as
news.  I never look at any news site now except Reddit.
[2]
 I know if something major
happens, or someone writes a particularly interesting article, it   
will show up there.  Why bother checking the front page of any
specific paper or magazine?  Reddit's like an RSS feed for the whole
web, with a filter for quality.  Similar sites include Digg, a technology news site that's
rapidly approaching Slashdot in popularity, and del.icio.us, the collaborative
bookmarking network that set off the "tagging" movement.  And whereas
Wikipedia's main appeal is that it's good enough and free, these
sites suggest that voters do a significantly better job than human
editors.The most dramatic example of Web 2.0 democracy is not in the selection
of ideas, but their production.  
I've noticed for a while that the stuff I read on individual people's
sites is as good as or better than the stuff I read in newspapers
and magazines.  And now I have independent evidence: the top links
on Reddit are generally links to individual people's sites rather  
than to magazine articles or news stories.My experience of writing
for magazines suggests an explanation.  Editors.  They control the
topics you can write about, and they can generally rewrite whatever
you produce.  The result is to damp extremes.  Editing yields 95th
percentile writing—95% of articles are improved by it, but 5% are
dragged down.  5% of the time you get "throngs of geeks."On the web, people can publish whatever they want.  Nearly all of
it falls short of the editor-damped writing in print publications.
But the pool of writers is very, very large.  If it's large enough,
the lack of damping means the best writing online should surpass  
the best in print.
[3]  
And now that the web has evolved mechanisms
for selecting good stuff, the web wins net.  Selection beats damping,
for the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the  
startups of the Bubble what bloggers are to the print media.  During
the Bubble, a startup meant a company headed by an MBA that was   
blowing through several million dollars of VC money to "get big
fast" in the most literal sense.  Now it means a smaller, younger, more technical group that just      
decided to make something great.  They'll decide later if they want  
to raise VC-scale funding, and if they take it, they'll take it on
their terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements
of "Web 2.0."  I also see a third: not to maltreat users.  During
the Bubble a lot of popular sites were quite high-handed with users.
And not just in obvious ways, like making them register, or subjecting
them to annoying ads.  The very design of the average site in the   
late 90s was an abuse.  Many of the most popular sites were loaded
with obtrusive branding that made them slow to load and sent the
user the message: this is our site, not yours.  (There's a physical
analog in the Intel and Microsoft stickers that come on some
laptops.)I think the root of the problem was that sites felt they were giving
something away for free, and till recently a company giving anything
away for free could be pretty high-handed about it.  Sometimes it
reached the point of economic sadism: site owners assumed that the
more pain they caused the user, the more benefit it must be to them.  
The most dramatic remnant of this model may be at salon.com, where   
you can read the beginning of a story, but to get the rest you have
sit through a movie.At Y Combinator we advise all the startups we fund never to lord
it over users.  Never make users register, unless you need to in
order to store something for them.  If you do make users register,   
never make them wait for a confirmation link in an email; in fact,
don't even ask for their email address unless you need it for some
reason.  Don't ask them any unnecessary questions.  Never send them
email unless they explicitly ask for it.  Never frame pages you
link to, or open them in new windows.  If you have a free version 
and a pay version, don't make the free version too restricted.  And
if you find yourself asking "should we allow users to do x?" just 
answer "yes" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups
never to let anyone fly under them, meaning never to let any other
company offer a cheaper, easier solution.  Another way to fly low 
is to give users more power.  Let users do what they want.  If you 
don't and a competitor does, you're in trouble.iTunes is Web 2.0ish in this sense.  Finally you can buy individual
songs instead of having to buy whole albums.  The recording industry
hated the idea and resisted it as long as possible.  But it was
obvious what users wanted, so Apple flew under the labels.
[4]
Though really it might be better to describe iTunes as Web 1.5.     
Web 2.0 applied to music would probably mean individual bands giving
away DRMless songs for free.The ultimate way to be nice to users is to give them something for
free that competitors charge for.  During the 90s a lot of people   
probably thought we'd have some working system for micropayments     
by now.  In fact things have gone in the other direction.  The most   
successful sites are the ones that figure out new ways to give stuff
away for free.  Craigslist has largely destroyed the classified ad
sites of the 90s, and OkCupid looks likely to do the same to the
previous generation of dating sites.Serving web pages is very, very cheap.  If you can make even a   
fraction of a cent per page view, you can make a profit.  And
technology for targeting ads continues to improve.  I wouldn't be
surprised if ten years from now eBay had been supplanted by an      
ad-supported freeBay (or, more likely, gBay).Odd as it might sound, we tell startups that they should try to
make as little money as possible.  If you can figure out a way to
turn a billion dollar industry into a fifty million dollar industry,
so much the better, if all fifty million go to you.  Though indeed,
making things cheaper often turns out to generate more money in the
end, just as automating things often turns out to generate more
jobs.The ultimate target is Microsoft.  What a bang that balloon is going
to make when someone pops it by offering a free web-based alternative 
to MS Office.
[5]
Who will?  Google?  They seem to be taking their
time.  I suspect the pin will be wielded by a couple of 20 year old
hackers who are too naive to be intimidated by the idea.  (How hard
can it be?)The Common ThreadAjax, democracy, and not dissing users.  What do they all have in  
common?  I didn't realize they had anything in common till recently,
which is one of the reasons I disliked the term "Web 2.0" so much.
It seemed that it was being used as a label for whatever happened
to be new—that it didn't predict anything.But there is a common thread.  Web 2.0 means using the web the way
it's meant to be used.  The "trends" we're seeing now are simply
the inherent nature of the web emerging from under the broken models
that got imposed on it during the Bubble.I realized this when I read an  interview with
Joe Kraus, the co-founder of Excite.
[6]

  Excite really never got the business model right at all.  We fell 
  into the classic problem of how when a new medium comes out it
  adopts the practices, the content, the business models of the old
  medium—which fails, and then the more appropriate models get
  figured out.

It may have seemed as if not much was happening during the years
after the Bubble burst.  But in retrospect, something was happening:
the web was finding its natural angle of repose.  The democracy 
component, for example—that's not an innovation, in the sense of
something someone made happen.  That's what the web naturally tends
to produce.Ditto for the idea of delivering desktop-like applications over the
web.  That idea is almost as old as the web.  But the first time    
around it was co-opted by Sun, and we got Java applets.  Java has
since been remade into a generic replacement for C++, but in 1996
the story about Java was that it represented a new model of software.
Instead of desktop applications, you'd run Java "applets" delivered
from a server.This plan collapsed under its own weight. Microsoft helped kill it,
but it would have died anyway.  There was no uptake among hackers.
When you find PR firms promoting
something as the next development platform, you can be sure it's
not.  If it were, you wouldn't need PR firms to tell you, because   
hackers would already be writing stuff on top of it, the way sites    
like Busmonster used Google Maps as a
platform before Google even meant it to be one.The proof that Ajax is the next hot platform is that thousands of  
hackers have spontaneously started building things on top
of it.  Mikey likes it.There's another thing all three components of Web 2.0 have in common.
Here's a clue.  Suppose you approached investors with the following
idea for a Web 2.0 startup:

  Sites like del.icio.us and flickr allow users to "tag" content
  with descriptive tokens.  But there is also huge source of
  implicit tags that they ignore: the text within web links.
  Moreover, these links represent a social network connecting the   
  individuals and organizations who created the pages, and by using
  graph theory we can compute from this network an estimate of the
  reputation of each member.  We plan to mine the web for these 
  implicit tags, and use them together with the reputation hierarchy
  they embody to enhance web searches.

How long do you think it would take them on average to realize that
it was a description of Google?Google was a pioneer in all three components of Web 2.0: their core
business sounds crushingly hip when described in Web 2.0 terms, 
"Don't maltreat users" is a subset of "Don't be evil," and of course
Google set off the whole Ajax boom with Google Maps.Web 2.0 means using the web as it was meant to be used, and Google
does.  That's their secret.    They're sailing with the wind, instead of sitting  
becalmed praying for a business model, like the print media, or   
trying to tack upwind by suing their customers, like Microsoft and 
the record labels.
[7]Google doesn't try to force things to happen their way.  They try   
to figure out what's going to happen, and arrange to be standing 
there when it does.  That's the way to approach technology—and 
as business includes an ever larger technological component, the
right way to do business.The fact that Google is a "Web 2.0" company shows that, while
meaningful, the term is also rather bogus.  It's like the word
"allopathic."  It just means doing things right, and it's a bad   
sign when you have a special word for that.
Notes[1]
From the conference
site, June 2004: "While the first wave of the Web was closely  
tied to the browser, the second wave extends applications across    
the web and enables a new generation of services and business
opportunities."  To the extent this means anything, it seems to be
about 
web-based applications.[2]
Disclosure: Reddit was funded by 
Y Combinator.  But although
I started using it out of loyalty to the home team, I've become a
genuine addict.  While we're at it, I'm also an investor in
!MSFT, having sold all my shares earlier this year.[3]
I'm not against editing. I spend more time editing than
writing, and I have a group of picky friends who proofread almost
everything I write.  What I dislike is editing done after the fact  
by someone else.[4]
Obvious is an understatement.  Users had been climbing in through  
the window for years before Apple finally moved the door.[5]
Hint: the way to create a web-based alternative to Office may
not be to write every component yourself, but to establish a protocol
for web-based apps to share a virtual home directory spread across
multiple servers.  Or it may be to write it all yourself.[6]
In Jessica Livingston's
Founders at
Work.[7]
Microsoft didn't sue their customers directly, but they seem 
to have done all they could to help SCO sue them.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, Peter
Norvig, Aaron Swartz, and Jeff Weiner for reading drafts of this, and to the
guys at O'Reilly and Adaptive Path for answering my questions.

Want to start a startup?  Get funded by
Y Combinator.




November 2009I don't think Apple realizes how badly the App Store approval process
is broken.  Or rather, I don't think they realize how much it matters
that it's broken.The way Apple runs the App Store has harmed their reputation with
programmers more than anything else they've ever done. 
Their reputation with programmers used to be great.
It used to be the most common complaint you heard
about Apple was that their fans admired them too uncritically.
The App Store has changed that.  Now a lot of programmers
have started to see Apple as evil.How much of the goodwill Apple once had with programmers have they
lost over the App Store?  A third?  Half?  And that's just so far.
The App Store is an ongoing karma leak.* * *How did Apple get into this mess?  Their fundamental problem is
that they don't understand software.They treat iPhone apps the way they treat the music they sell through
iTunes.  Apple is the channel; they own the user; if you want to
reach users, you do it on their terms. The record labels agreed,
reluctantly.  But this model doesn't work for software.  It doesn't
work for an intermediary to own the user.  The software business
learned that in the early 1980s, when companies like VisiCorp showed
that although the words "software" and "publisher" fit together,
the underlying concepts don't.  Software isn't like music or books.
It's too complicated for a third party to act as an intermediary
between developer and user.   And yet that's what Apple is trying
to be with the App Store: a software publisher.  And a particularly
overreaching one at that, with fussy tastes and a rigidly enforced
house style.If software publishing didn't work in 1980, it works even less now
that software development has evolved from a small number of big
releases to a constant stream of small ones.  But Apple doesn't
understand that either.  Their model of product development derives
from hardware.  They work on something till they think it's finished,
then they release it.  You have to do that with hardware, but because
software is so easy to change, its design can benefit from evolution.
The standard way to develop applications now is to launch fast and
iterate.  Which means it's a disaster to have long, random delays
each time you release a new version.Apparently Apple's attitude is that developers should be more careful
when they submit a new version to the App Store.  They would say
that.  But powerful as they are, they're not powerful enough to
turn back the evolution of technology.  Programmers don't use
launch-fast-and-iterate out of laziness.  They use it because it
yields the best results.  By obstructing that process, Apple is
making them do bad work, and programmers hate that as much as Apple
would.How would Apple like it if when they discovered a serious bug in
OS X, instead of releasing a software update immediately, they had
to submit their code to an intermediary who sat on it for a month
and then rejected it because it contained an icon they didn't like?By breaking software development, Apple gets the opposite of what
they intended: the version of an app currently available in the App
Store tends to be an old and buggy one.  One developer told me:

  As a result of their process, the App Store is full of half-baked
  applications. I make a new version almost every day that I release
  to beta users. The version on the App Store feels old and crappy.
  I'm sure that a lot of developers feel this way: One emotion is
  "I'm not really proud about what's in the App Store", and it's
  combined with the emotion "Really, it's Apple's fault."

Another wrote:

  I believe that they think their approval process helps users by
  ensuring quality.  In reality, bugs like ours get through all the
  time and then it can take 4-8 weeks to get that bug fix approved,
  leaving users to think that iPhone apps sometimes just don't work.
  Worse for Apple, these apps work just fine on other platforms
  that have immediate approval processes.

Actually I suppose Apple has a third misconception: that all the
complaints about App Store approvals are not a serious problem.
They must hear developers complaining.  But partners and suppliers
are always complaining.  It would be a bad sign if they weren't;
it would mean you were being too easy on them.  Meanwhile the iPhone
is selling better than ever.  So why do they need to fix anything?They get away with maltreating developers, in the short term, because
they make such great hardware.  I just bought a new 27" iMac a
couple days ago.  It's fabulous.  The screen's too shiny, and the
disk is surprisingly loud, but it's so beautiful that you can't
make yourself care.So I bought it, but I bought it, for the first time, with misgivings.
I felt the way I'd feel buying something made in a country with a
bad human rights record.  That was new.  In the past when I bought
things from Apple it was an unalloyed pleasure.  Oh boy!  They make
such great stuff.  This time it felt like a Faustian bargain.  They
make such great stuff, but they're such assholes.  Do I really want
to support this company?* * *Should Apple care what people like me think?  What difference does
it make if they alienate a small minority of their users?There are a couple reasons they should care.  One is that these
users are the people they want as employees.  If your company seems
evil, the best programmers won't work for you.  That hurt Microsoft
a lot starting in the 90s.  Programmers started to feel sheepish
about working there.  It seemed like selling out.  When people from
Microsoft were talking to other programmers and they mentioned where
they worked, there were a lot of self-deprecating jokes about having
gone over to the dark side.  But the real problem for Microsoft
wasn't the embarrassment of the people they hired.  It was the
people they never got.  And you know who got them?  Google and
Apple.  If Microsoft was the Empire, they were the Rebel Alliance.
And it's largely because they got more of the best people that
Google and Apple are doing so much better than Microsoft today.Why are programmers so fussy about their employers' morals?  Partly
because they can afford to be.  The best programmers can work
wherever they want.  They don't have to work for a company they
have qualms about.But the other reason programmers are fussy, I think, is that evil
begets stupidity.  An organization that wins by exercising power
starts to lose the ability to win by doing better work.  And it's
not fun for a smart person to work in a place where the best ideas
aren't the ones that win.  I think the reason Google embraced "Don't
be evil" so eagerly was not so much to impress the outside world
as to inoculate themselves against arrogance.
[1]That has worked for Google so far.  They've become more
bureaucratic, but otherwise they seem to have held true to their
original principles. With Apple that seems less the case.  When you
look at the famous 
1984 ad 
now, it's easier to imagine Apple as the
dictator on the screen than the woman with the hammer.
[2]
In fact, if you read the dictator's speech it sounds uncannily like a
prophecy of the App Store.

  We have triumphed over the unprincipled dissemination of facts.We have created, for the first time in all history, a garden of
  pure ideology, where each worker may bloom secure from the pests
  of contradictory and confusing truths.

The other reason Apple should care what programmers think of them
is that when you sell a platform, developers make or break you.  If
anyone should know this, Apple should.  VisiCalc made the Apple II.And programmers build applications for the platforms they use.  Most
applications—most startups, probably—grow out of personal projects.
Apple itself did.  Apple made microcomputers because that's what
Steve Wozniak wanted for himself.  He couldn't have afforded a
minicomputer. 
[3]
 Microsoft likewise started out making interpreters
for little microcomputers because
Bill Gates and Paul Allen were interested in using them.  It's a
rare startup that doesn't build something the founders use.The main reason there are so many iPhone apps is that so many programmers
have iPhones.  They may know, because they read it in an article,
that Blackberry has such and such market share.  But in practice
it's as if RIM didn't exist. If they're going to build something,
they want to be able to use it themselves, and that means building
an iPhone app.So programmers continue to develop iPhone apps, even though Apple
continues to maltreat them.  They're like someone stuck in an abusive
relationship.  They're so attracted to the iPhone that they can't
leave.  But they're looking for a way out.  One wrote:

  While I did enjoy developing for the iPhone, the control they
  place on the App Store does not give me the drive to develop
  applications as I would like. In fact I don't intend to make any
  more iPhone applications unless absolutely necessary.
[4]

Can anything break this cycle?  No device I've seen so far could.
Palm and RIM haven't a hope.  The only credible contender is Android.
But Android is an orphan; Google doesn't really care about it, not
the way Apple cares about the iPhone.  Apple cares about the iPhone
the way Google cares about search.* * *Is the future of handheld devices one locked down by Apple?  It's
a worrying prospect.  It would be a bummer to have another grim
monoculture like we had in the 1990s.  In 1995, writing software
for end users was effectively identical with writing Windows
applications.  Our horror at that prospect was the single biggest
thing that drove us to start building web apps.At least we know now what it would take to break Apple's lock.
You'd have to get iPhones out of programmers' hands.  If programmers
used some other device for mobile web access, they'd start to develop
apps for that instead.How could you make a device programmers liked better than the iPhone?
It's unlikely you could make something better designed.  Apple
leaves no room there.  So this alternative device probably couldn't
win on general appeal.  It would have to win by virtue of some
appeal it had to programmers specifically.One way to appeal to programmers is with software.  If you
could think of an application programmers had to have, but that
would be impossible in the circumscribed world of the iPhone, 
you could presumably get them to switch.That would definitely happen if programmers started to use handhelds
as development machines—if handhelds displaced laptops the
way laptops displaced desktops.  You need more control of a development
machine than Apple will let you have over an iPhone.Could anyone make a device that you'd carry around in your pocket
like a phone, and yet would also work as a development machine?
It's hard to imagine what it would look like.  But I've learned
never to say never about technology.  A phone-sized device that
would work as a development machine is no more miraculous by present
standards than the iPhone itself would have seemed by the standards
of 1995.My current development machine is a MacBook Air, which I use with
an external monitor and keyboard in my office, and by itself when
traveling.  If there was a version half the size I'd prefer it.
That still wouldn't be small enough to carry around everywhere like
a phone, but we're within a factor of 4 or so.  Surely that gap is
bridgeable.  In fact, let's make it an
RFS. Wanted: 
Woman with hammer.Notes[1]
When Google adopted "Don't be evil," they were still so small
that no one would have expected them to be, yet.
[2]
The dictator in the 1984 ad isn't Microsoft, incidentally;
it's IBM.  IBM seemed a lot more frightening in those days, but
they were friendlier to developers than Apple is now.[3]
He couldn't even afford a monitor.  That's why the Apple
I used a TV as a monitor.[4]
Several people I talked to mentioned how much they liked the
iPhone SDK.  The problem is not Apple's products but their policies.
Fortunately policies are software; Apple can change them instantly
if they want to.  Handy that, isn't it?Thanks to Sam Altman, Trevor Blackwell, Ross Boucher, 
James Bracy, Gabor Cselle,
Patrick Collison, Jason Freedman, John Gruber, Joe Hewitt, Jessica Livingston,
Robert Morris, Teng Siong Ong, Nikhil Pandit, Savraj Singh, and Jared Tame for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




July 2004(This essay is derived from a talk at Oscon 2004.)
A few months ago I finished a new 
book, 
and in reviews I keep
noticing words like "provocative'' and "controversial.'' To say
nothing of "idiotic.''I didn't mean to make the book controversial.  I was trying to make
it efficient.  I didn't want to waste people's time telling them
things they already knew.  It's more efficient just to give them
the diffs.  But I suppose that's bound to yield an alarming book.EdisonsThere's no controversy about which idea is most controversial:
the suggestion that variation in wealth might not be as big a
problem as we think.I didn't say in the book that variation in wealth was in itself a
good thing.  I said in some situations it might be a sign of good
things.  A throbbing headache is not a good thing, but it can be
a sign of a good thing-- for example, that you're recovering
consciousness after being hit on the head.Variation in wealth can be a sign of variation in productivity.
(In a society of one, they're identical.) And that
is almost certainly a good thing: if your society has no variation
in productivity, it's probably not because everyone is Thomas
Edison.  It's probably because you have no Thomas Edisons.In a low-tech society you don't see much variation in productivity.
If you have a tribe of nomads collecting sticks for a fire, how
much more productive is the best stick gatherer going to be than
the worst?  A factor of two?  Whereas when you hand people a complex tool
like a computer, the variation in what they can do with
it is enormous.That's not a new idea.  Fred Brooks wrote about it in 1974, and
the study he quoted was published in 1968.  But I think he
underestimated the variation between programmers.  He wrote about productivity in lines
of code:  the best programmers can solve a given problem in a tenth
the time.  But what if the problem isn't given? In programming, as
in many fields, the hard part isn't solving problems, but deciding
what problems to solve.  Imagination is hard to measure, but
in practice it dominates the kind of productivity that's measured
in lines of code.Productivity varies in any field, but there are few in which it
varies so much.  The variation between programmers
is so great that it becomes a difference in kind.  I don't
think this is something intrinsic to programming, though.  In every field,
technology magnifies differences in productivity.  I think what's
happening in programming is just that we have a lot of technological
leverage.  But in every field the lever is getting longer, so the
variation we see is something that more and more fields will see
as time goes on.  And the success of companies, and countries, will
depend increasingly on how they deal with it.If variation in productivity increases with technology, then the
contribution of the most productive individuals will not only be
disproportionately large, but will actually grow with time.  When
you reach the point where 90% of a group's output is created by 1%
of its members, you lose big if something (whether Viking raids,
or central planning) drags their productivity down to the average.If we want to get the most out of them, we need to understand these
especially productive people.  What motivates them?  What do they
need to do their jobs?  How do you recognize them? How do you
get them to come and work for you?  And then of course there's the
question, how do you become one?More than MoneyI know a handful of super-hackers, so I sat down and thought about
what they have in common.  Their defining quality is probably that
they really love to program.  Ordinary programmers write code to pay
the bills.  Great hackers think of it as something they do for fun,
and which they're delighted to find people will pay them for.Great programmers are sometimes said to be indifferent to money.
This isn't quite true.  It is true that all they really care about
is doing interesting work.  But if you make enough money, you get
to work on whatever you want, and for that reason hackers are
attracted by the idea of making really large amounts of money.
But as long as they still have to show up for work every day, they
care more about what they do there than how much they get paid for
it.Economically, this is a fact of the greatest importance, because
it means you don't have to pay great hackers anything like what
they're worth.  A great programmer might be ten or a hundred times
as productive as an ordinary one, but he'll consider himself lucky
to get paid three times as much.  As I'll explain later, this is
partly because great hackers don't know how good they are.  But
it's also because money is not the main thing they want.What do hackers want?  Like all craftsmen, hackers like good tools.
In fact, that's an understatement.  Good hackers find it unbearable
to use bad tools.  They'll simply refuse to work on projects with
the wrong infrastructure.At a startup I once worked for, one of the things pinned up on our
bulletin board was an ad from IBM.  It was a picture of an AS400,
and the headline read, I think, "hackers despise
it.'' [1]When you decide what infrastructure to use for a project, you're
not just making a technical decision.  You're also making a social
decision, and this may be the more important of the two.  For
example, if your company wants to write some software, it might
seem a prudent choice to write it in Java.  But when you choose a
language, you're also choosing a community.  The programmers you'll
be able to hire to work on a Java project won't be as
smart as the
ones you could get to work on a project written in Python.
And the quality of your hackers probably matters more than the
language you choose.  Though, frankly, the fact that good hackers
prefer Python to Java should tell you something about the relative
merits of those languages.Business types prefer the most popular languages because they view
languages as standards. They don't want to bet the company on
Betamax.  The thing about languages, though, is that they're not
just standards.  If you have to move bits over a network, by all
means use TCP/IP.  But a programming language isn't just a format.
A programming language is a medium of expression.I've read that Java has just overtaken Cobol as the most popular
language.  As a standard, you couldn't wish for more.  But as a
medium of expression, you could do a lot better.  Of all the great
programmers I can think of, I know of only one who would voluntarily
program in Java.  And of all the great programmers I can think of
who don't work for Sun, on Java, I know of zero.Great hackers also generally insist on using open source software.
Not just because it's better, but because it gives them more control.
Good hackers insist on control.  This is part of what makes them
good hackers:  when something's broken, they need to fix it.  You
want them to feel this way about the software they're writing for
you.  You shouldn't be surprised when they feel the same way about
the operating system.A couple years ago a venture capitalist friend told me about a new
startup he was involved with.  It sounded promising.  But the next
time I talked to him, he said they'd decided to build their software
on Windows NT, and had just hired a very experienced NT developer
to be their chief technical officer.  When I heard this, I thought,
these guys are doomed.  One, the CTO couldn't be a first rate
hacker, because to become an eminent NT developer he would have
had to use NT voluntarily, multiple times, and I couldn't imagine
a great hacker doing that; and two, even if he was good, he'd have
a hard time hiring anyone good to work for him if the project had
to be built on NT. [2]The Final FrontierAfter software, the most important tool to a hacker is probably
his office.  Big companies think the function of office space is to express
rank.  But hackers use their offices for more than that: they
use their office as a place to think in.  And if you're a technology
company, their thoughts are your product.  So making hackers work
in a noisy, distracting environment is like having a paint factory
where the air is full of soot.The cartoon strip Dilbert has a lot to say about cubicles, and with
good reason.  All the hackers I know despise them.  The mere prospect
of being interrupted is enough to prevent hackers from working on
hard problems.  If you want to get real work done in an office with
cubicles, you have two options: work at home, or come in early or
late or on a weekend, when no one else is there.  Don't companies
realize this is a sign that something is broken?  An office
environment is supposed to be something that helps
you work, not something you work despite.Companies like Cisco are proud that everyone there has a cubicle,
even the CEO.  But they're not so advanced as they think; obviously
they still view office space as a badge of rank.  Note too that
Cisco is famous for doing very little product development in house.
They get new technology by buying the startups that created it-- where
presumably the hackers did have somewhere quiet to work.One big company that understands what hackers need is Microsoft.
I once saw a recruiting ad for Microsoft with a big picture of a
door.  Work for us, the premise was, and we'll give you a place to
work where you can actually get work done.   And you know, Microsoft
is remarkable among big companies in that they are able to develop
software in house.  Not well, perhaps, but well enough.If companies want hackers to be productive, they should look at
what they do at home.  At home, hackers can arrange things themselves
so they can get the most done.  And when they work at home, hackers
don't work in noisy, open spaces; they work in rooms with doors.  They
work in cosy, neighborhoody places with people around and somewhere
to walk when they need to mull something over, instead of in glass
boxes set in acres of parking lots.  They have a sofa they can take
a nap on when they feel tired, instead of sitting in a coma at
their desk, pretending to work.  There's no crew of people with
vacuum cleaners that roars through every evening during the prime
hacking hours.  There are no meetings or, God forbid, corporate
retreats or team-building exercises.  And when you look at what
they're doing on that computer, you'll find it reinforces what I
said earlier about tools.  They may have to use Java and Windows
at work, but at home, where they can choose for themselves, you're
more likely to find them using Perl and Linux.Indeed, these statistics about Cobol or Java being the most popular
language can be misleading.  What we ought to look at, if we want
to know what tools are best, is what hackers choose when they can
choose freely-- that is, in projects of their own.  When you ask
that question, you find that open source operating systems already
have a dominant market share, and the number one language is probably
Perl.InterestingAlong with good tools, hackers want interesting projects.  What
makes a project interesting?  Well, obviously overtly sexy
applications like stealth planes or special effects software would
be interesting to work on.  But any application can be interesting
if it poses novel technical challenges.  So it's hard to predict
which problems hackers will like, because some become
interesting only when the people working on them discover a new
kind of solution.  Before ITA
(who wrote the software inside Orbitz),
the people working on airline fare searches probably thought it
was one of the most boring applications imaginable.  But ITA made
it interesting by 
redefining the problem in a more ambitious way.I think the same thing happened at Google.  When Google was founded,
the conventional wisdom among the so-called portals was that search
was boring and unimportant.  But the guys at Google didn't think
search was boring, and that's why they do it so well.This is an area where managers can make a difference.  Like a parent
saying to a child, I bet you can't clean up your whole room in
ten minutes, a good manager can sometimes redefine a problem as a
more interesting one.  Steve Jobs seems to be particularly good at
this, in part simply by having high standards.  There were a lot
of small, inexpensive computers before the Mac.  He redefined the
problem as: make one that's beautiful.  And that probably drove
the developers harder than any carrot or stick could.They certainly delivered.  When the Mac first appeared, you didn't
even have to turn it on to know it would be good; you could tell
from the case.  A few weeks ago I was walking along the street in
Cambridge, and in someone's trash I saw what appeared to be a Mac
carrying case.  I looked inside, and there was a Mac SE.  I carried
it home and plugged it in, and it booted.  The happy Macintosh
face, and then the finder.  My God, it was so simple.  It was just
like ... Google.Hackers like to work for people with high standards.  But it's not
enough just to be exacting.  You have to insist on the right things.
Which usually means that you have to be a hacker yourself.  I've
seen occasional articles about how to manage programmers.  Really
there should be two articles: one about what to do if
you are yourself a programmer, and one about what to do if you're not.  And the 
second could probably be condensed into two words:  give up.The problem is not so much the day to day management.  Really good
hackers are practically self-managing.  The problem is, if you're
not a hacker, you can't tell who the good hackers are.  A similar
problem explains why American cars are so ugly.  I call it the
design paradox.  You might think that you could make your products
beautiful just by hiring a great designer to design them.  But if
you yourself don't have good taste, 
how are you going to recognize
a good designer?  By definition you can't tell from his portfolio.
And you can't go by the awards he's won or the jobs he's had,
because in design, as in most fields, those tend to be driven by
fashion and schmoozing, with actual ability a distant third.
There's no way around it:  you can't manage a process intended to
produce beautiful things without knowing what beautiful is.  American
cars are ugly because American car companies are run by people with
bad taste.Many people in this country think of taste as something elusive,
or even frivolous.  It is neither.  To drive design, a manager must
be the most demanding user of a company's products.  And if you
have really good taste, you can, as Steve Jobs does, make satisfying
you the kind of problem that good people like to work on.Nasty Little ProblemsIt's pretty easy to say what kinds of problems are not interesting:
those where instead of solving a few big, clear, problems, you have
to solve a lot of nasty little ones.  One of the worst kinds of
projects is writing an interface to a piece of software that's
full of bugs.  Another is when you have to customize
something for an individual client's complex and ill-defined needs.
To hackers these kinds of projects are the death of a thousand
cuts.The distinguishing feature of nasty little problems is that you
don't learn anything from them.   Writing a compiler is interesting
because it teaches you what a compiler is.  But writing an interface
to a buggy piece of software doesn't teach you anything, because the
bugs are random.  [3] So it's not just fastidiousness that makes good
hackers avoid nasty little problems.  It's more a question of
self-preservation.  Working on nasty little problems makes you
stupid.  Good hackers avoid it for the same reason models avoid
cheeseburgers.Of course some problems inherently have this character.  And because
of supply and demand, they pay especially well.  So a company that
found a way to get great hackers to work on tedious problems would
be very successful.  How would you do it?One place this happens is in startups.  At our startup we had 
Robert Morris working as a system administrator.  That's like having the
Rolling Stones play at a bar mitzvah.  You can't hire that kind of
talent.  But people will do any amount of drudgery for companies
of which they're the founders.  [4]Bigger companies solve the problem by partitioning the company.
They get smart people to work for them by establishing a separate
R&D department where employees don't have to work directly on
customers' nasty little problems. [5] In this model, the research
department functions like a mine. They produce new ideas; maybe
the rest of the company will be able to use them.You may not have to go to this extreme.  
Bottom-up programming
suggests another way to partition the company: have the smart people
work as toolmakers.  If your company makes software to do x, have
one group that builds tools for writing software of that type, and
another that uses these tools to write the applications.  This way
you might be able to get smart people to write 99% of your code,
but still keep them almost as insulated from users as they would
be in a traditional research department.  The toolmakers would have
users, but they'd only be the company's own developers.  [6]If Microsoft used this approach, their software wouldn't be so full
of security holes, because the less smart people writing the actual
applications wouldn't be doing low-level stuff like allocating
memory.  Instead of writing Word directly in C, they'd be plugging
together big Lego blocks of Word-language.  (Duplo, I believe, is
the technical term.)ClumpingAlong with interesting problems, what good hackers like is other
good hackers.  Great hackers tend to clump together-- sometimes
spectacularly so, as at Xerox Parc.   So you won't attract good
hackers in linear proportion to how good an environment you create
for them.  The tendency to clump means it's more like the square
of the environment.  So it's winner take all.  At any given time,
there are only about ten or twenty places where hackers most want to
work, and if you aren't one of them, you won't just have fewer
great hackers, you'll have zero.Having great hackers is not, by itself, enough to make a company
successful.  It works well for Google and ITA, which are two of
the hot spots right now, but it didn't help Thinking Machines or
Xerox.  Sun had a good run for a while, but their business model
is a down elevator.  In that situation, even the best hackers can't
save you.I think, though, that all other things being equal, a company that
can attract great hackers will have a huge advantage.  There are
people who would disagree with this.  When we were making the rounds
of venture capital firms in the 1990s, several told us that software
companies didn't win by writing great software, but through brand,
and dominating channels, and doing the right deals.They really seemed to believe this, and I think I know why.  I
think what a lot of VCs are looking for, at least unconsciously,
is the next Microsoft.  And of course if Microsoft is your model,
you shouldn't be looking for companies that hope to win by writing
great software.  But VCs are mistaken to look for the next Microsoft,
because no startup can be the next Microsoft unless some other
company is prepared to bend over at just the right moment and be
the next IBM.It's a mistake to use Microsoft as a model, because their whole
culture derives from that one lucky break.  Microsoft is a bad data
point.  If you throw them out, you find that good products do tend
to win in the market.  What VCs should be looking for is the next
Apple, or the next Google.I think Bill Gates knows this.  What worries him about Google is
not the power of their brand, but the fact that they have
better hackers. [7]
RecognitionSo who are the great hackers?  How do you know when you meet one?
That turns out to be very hard.  Even hackers can't tell.  I'm
pretty sure now that my friend Trevor Blackwell is a great hacker.
You may have read on Slashdot how he made his 
own Segway.  The
remarkable thing about this project was that he wrote all the
software in one day (in Python, incidentally).For Trevor, that's
par for the course.  But when I first met him, I thought he was a
complete idiot.  He was standing in Robert Morris's office babbling
at him about something or other, and I remember standing behind
him making frantic gestures at Robert to shoo this nut out of his
office so we could go to lunch.  Robert says he misjudged Trevor
at first too.  Apparently when Robert first met him, Trevor had
just begun a new scheme that involved writing down everything about
every aspect of his life on a stack of index cards, which he carried
with him everywhere.  He'd also just arrived from Canada, and had
a strong Canadian accent and a mullet.The problem is compounded by the fact that hackers, despite their
reputation for social obliviousness, sometimes put a good deal of
effort into seeming smart.  When I was in grad school I used to
hang around the MIT AI Lab occasionally. It was kind of intimidating
at first.  Everyone there spoke so fast.  But after a while I
learned the trick of speaking fast.  You don't have to think any
faster; just use twice as many words to say everything.  With this amount of noise in the signal, it's hard to tell good
hackers when you meet them.  I can't tell, even now.  You also
can't tell from their resumes.  It seems like the only way to judge
a hacker is to work with him on something.And this is the reason that high-tech areas 
only happen around universities.  The active ingredient
here is not so much the professors as the students.  Startups grow up
around universities because universities bring together promising young
people and make them work on the same projects.  The
smart ones learn who the other smart ones are, and together
they cook up new projects of their own.Because you can't tell a great hacker except by working with him,
hackers themselves can't tell how good they are.  This is true to
a degree in most fields.  I've found that people who
are great at something are not so much convinced of their own
greatness as mystified at why everyone else seems so incompetent.
But it's particularly hard for hackers to know how good they are,
because it's hard to compare their work.  This is easier in most
other fields.  In the hundred meters, you know in 10 seconds who's
fastest.  Even in math there seems to be a general consensus about
which problems are hard to solve, and what constitutes a good
solution.  But hacking is like writing.  Who can say which of two
novels is better?  Certainly not the authors.With hackers, at least, other hackers can tell.  That's because,
unlike novelists, hackers collaborate on projects.  When you get
to hit a few difficult problems over the net at someone, you learn
pretty quickly how hard they hit them back.  But hackers can't
watch themselves at work.  So if you ask a great hacker how good
he is, he's almost certain to reply, I don't know.  He's not just
being modest.  He really doesn't know.And none of us know, except about people we've actually worked
with.  Which puts us in a weird situation: we don't know who our
heroes should be.  The hackers who become famous tend to become
famous by random accidents of PR.  Occasionally I need to give an
example of a great hacker, and I never know who to use.  The first
names that come to mind always tend to be people I know personally,
but it seems lame to use them.  So, I think, maybe I should say
Richard Stallman, or Linus Torvalds, or Alan Kay, or someone famous
like that.  But I have no idea if these guys are great hackers.
I've never worked with them on anything.If there is a Michael Jordan of hacking, no one knows, including
him.CultivationFinally, the question the hackers have all been wondering about:
how do you become a great hacker?  I don't know if it's possible
to make yourself into one.  But it's certainly possible to do things
that make you stupid, and if you can make yourself stupid, you
can probably make yourself smart too.The key to being a good hacker may be to work on what you like.
When I think about the great hackers I know, one thing they have
in common is the extreme 
difficulty of making them work 
on anything they
don't want to.  I don't know if this is cause or effect; it may be
both.To do something well you have to love it.  
So to the extent you
can preserve hacking as something you love, you're likely to do it
well.  Try to keep the sense of wonder you had about programming at
age 14.  If you're worried that your current job is rotting your
brain, it probably is.The best hackers tend to be smart, of course, but that's true in
a lot of fields.  Is there some quality that's unique to hackers?
I asked some friends, and the number one thing they mentioned was
curiosity.  
I'd always supposed that all smart people were curious--
that curiosity was simply the first derivative of knowledge.  But
apparently hackers are particularly curious, especially about how
things work.  That makes sense, because programs are in effect
giant descriptions of how things work.Several friends mentioned hackers' ability to concentrate-- their
ability, as one put it, to "tune out everything outside their own
heads.''  I've certainly noticed this.  And I've heard several 
hackers say that after drinking even half a beer they can't program at
all.   So maybe hacking does require some special ability to focus.
Perhaps great hackers can load a large amount of context into their
head, so that when they look at a line of code, they see not just
that line but the whole program around it.  John McPhee
wrote that Bill Bradley's success as a basketball player was due
partly to his extraordinary peripheral vision.  "Perfect'' eyesight
means about 47 degrees of vertical peripheral vision.  Bill Bradley
had 70; he could see the basket when he was looking at the floor.
Maybe great hackers have some similar inborn ability.  (I cheat by
using a very dense language, 
which shrinks the court.)This could explain the disconnect over cubicles.  Maybe the people
in charge of facilities, not having any concentration to shatter,
have no idea that working in a cubicle feels to a hacker like having
one's brain in a blender.  (Whereas Bill, if the rumors of autism
are true, knows all too well.)One difference I've noticed between great hackers and smart people
in general is that hackers are more 
politically incorrect.  To the
extent there is a secret handshake among good hackers, it's when they
know one another well enough to express opinions that would get
them stoned to death by the general public.  And I can see why
political incorrectness would be a useful quality in programming.
Programs are very complex and, at least in the hands of good
programmers, very fluid.  In such situations it's helpful to have
a habit of questioning assumptions.Can you cultivate these qualities?  I don't know.  But you can at
least not repress them.  So here is my best shot at a recipe.  If
it is possible to make yourself into a great hacker, the way to do
it may be to make the following deal with yourself: you never have
to work on boring projects (unless your family will starve otherwise),
and in return, you'll never allow yourself to do a half-assed job.
All the great hackers I know seem to have made that deal, though
perhaps none of them had any choice in the matter.Notes
[1] In fairness, I have to say that IBM makes decent hardware.  I
wrote this on an IBM laptop.[2] They did turn out to be doomed.  They shut down a few months
later.[3] I think this is what people mean when they talk
about the "meaning of life."  On the face of it, this seems an 
odd idea.  Life isn't an expression; how could it have meaning?
But it can have a quality that feels a lot like meaning.  In a project
like a compiler, you have to solve a lot of problems, but the problems
all fall into a pattern, as in a signal.  Whereas when the problems
you have to solve are random, they seem like noise.
[4] Einstein at one point worked designing refrigerators. (He had equity.)[5] It's hard to say exactly what constitutes research in the
computer world, but as a first approximation, it's software that
doesn't have users.I don't think it's publication that makes the best hackers want to work
in research departments.  I think it's mainly not having to have a
three hour meeting with a product manager about problems integrating
the Korean version of Word 13.27 with the talking paperclip.[6] Something similar has been happening for a long time in the
construction industry. When you had a house built a couple hundred
years ago, the local builders built everything in it.  But increasingly
what builders do is assemble components designed and manufactured
by someone else.  This has, like the arrival of desktop publishing,
given people the freedom to experiment in disastrous ways, but it
is certainly more efficient.[7] Google is much more dangerous to Microsoft than Netscape was.
Probably more dangerous than any other company has ever been.  Not
least because they're determined to fight.  On their job listing
page, they say that one of their "core values'' is "Don't be evil.''
From a company selling soybean oil or mining equipment, such a
statement would merely be eccentric.  But I think all of us in the
computer world recognize who that is a declaration of war on.Thanks to Jessica Livingston, Robert Morris, and Sarah Harlin
for reading earlier versions of this talk.May 2021There's one kind of opinion I'd be very afraid to express publicly.
If someone I knew to be both a domain expert and a reasonable person
proposed an idea that sounded preposterous, I'd be very reluctant
to say "That will never work."Anyone who has studied the history of ideas, and especially the
history of science, knows that's how big things start. Someone
proposes an idea that sounds crazy, most people dismiss it, then
it gradually takes over the world.Most implausible-sounding ideas are in fact bad and could be safely
dismissed. But not when they're proposed by reasonable domain
experts. If the person proposing the idea is reasonable, then they
know how implausible it sounds. And yet they're proposing it anyway.
That suggests they know something you don't. And if they have deep
domain expertise, that's probably the source of it.
[1]Such ideas are not merely unsafe to dismiss, but disproportionately
likely to be interesting. When the average person proposes an
implausible-sounding idea, its implausibility is evidence of their
incompetence. But when a reasonable domain expert does it, the
situation is reversed. There's something like an efficient market
here: on average the ideas that seem craziest will, if correct,
have the biggest effect. So if you can eliminate the theory that
the person proposing an implausible-sounding idea is incompetent,
its implausibility switches from evidence that it's boring to
evidence that it's exciting.
[2]Such ideas are not guaranteed to work. But they don't have to be.
They just have to be sufficiently good bets — to have sufficiently
high expected value. And I think on average they do. I think if you
bet on the entire set of implausible-sounding ideas proposed by
reasonable domain experts, you'd end up net ahead.The reason is that everyone is too conservative. The word "paradigm"
is overused, but this is a case where it's warranted. Everyone is
too much in the grip of the current paradigm. Even the people who
have the new ideas undervalue them initially. Which means that
before they reach the stage of proposing them publicly, they've
already subjected them to an excessively strict filter.
[3]The wise response to such an idea is not to make statements, but
to ask questions, because there's a real mystery here. Why has this
smart and reasonable person proposed an idea that seems so wrong?
Are they mistaken, or are you? One of you has to be. If you're the
one who's mistaken, that would be good to know, because it means
there's a hole in your model of the world. But even if they're
mistaken, it should be interesting to learn why. A trap that an
expert falls into is one you have to worry about too.This all seems pretty obvious. And yet there are clearly a lot of
people who don't share my fear of dismissing new ideas. Why do they
do it? Why risk looking like a jerk now and a fool later, instead
of just reserving judgement?One reason they do it is envy. If you propose a radical new idea
and it succeeds, your reputation (and perhaps also your wealth)
will increase proportionally. Some people would be envious if that
happened, and this potential envy propagates back into a conviction
that you must be wrong.Another reason people dismiss new ideas is that it's an easy way
to seem sophisticated. When a new idea first emerges, it usually
seems pretty feeble. It's a mere hatchling. Received wisdom is a
full-grown eagle by comparison. So it's easy to launch a devastating
attack on a new idea, and anyone who does will seem clever to those
who don't understand this asymmetry.This phenomenon is exacerbated by the difference between how those
working on new ideas and those attacking them are rewarded. The
rewards for working on new ideas are weighted by the value of the
outcome. So it's worth working on something that only has a 10%
chance of succeeding if it would make things more than 10x better.
Whereas the rewards for attacking new ideas are roughly constant;
such attacks seem roughly equally clever regardless of the target.People will also attack new ideas when they have a vested interest
in the old ones. It's not surprising, for example, that some of
Darwin's harshest critics were churchmen. People build whole careers
on some ideas. When someone claims they're false or obsolete, they
feel threatened.The lowest form of dismissal is mere factionalism: to automatically
dismiss any idea associated with the opposing faction. The lowest
form of all is to dismiss an idea because of who proposed it.But the main thing that leads reasonable people to dismiss new ideas
is the same thing that holds people back from proposing them: the
sheer pervasiveness of the current paradigm. It doesn't just affect
the way we think; it is the Lego blocks we build thoughts out of.
Popping out of the current paradigm is something only a few people
can do. And even they usually have to suppress their intuitions at
first, like a pilot flying through cloud who has to trust his
instruments over his sense of balance.
[4]Paradigms don't just define our present thinking. They also vacuum
up the trail of crumbs that led to them, making our standards for
new ideas impossibly high. The current paradigm seems so perfect
to us, its offspring, that we imagine it must have been accepted
completely as soon as it was discovered — that whatever the church thought
of the heliocentric model, astronomers must have been convinced as
soon as Copernicus proposed it. Far, in fact, from it. Copernicus
published the heliocentric model in 1532, but it wasn't till the
mid seventeenth century that the balance of scientific opinion
shifted in its favor.
[5]Few understand how feeble new ideas look when they first appear.
So if you want to have new ideas yourself, one of the most valuable
things you can do is to learn what they look like when they're born.
Read about how new ideas happened, and try to get yourself into the
heads of people at the time. How did things look to them, when the
new idea was only half-finished, and even the person who had it was
only half-convinced it was right?But you don't have to stop at history. You can observe big new ideas
being born all around you right now. Just look for a reasonable
domain expert proposing something that sounds wrong.If you're nice, as well as wise, you won't merely resist attacking
such people, but encourage them. Having new ideas is a lonely
business. Only those who've tried it know how lonely. These people
need your help. And if you help them, you'll probably learn something
in the process.Notes[1]
This domain expertise could be in another field. Indeed,
such crossovers tend to be particularly promising.[2]
I'm not claiming this principle extends much beyond math,
engineering, and the hard sciences. In politics, for example,
crazy-sounding ideas generally are as bad as they sound. Though
arguably this is not an exception, because the people who propose
them are not in fact domain experts; politicians are domain experts
in political tactics, like how to get elected and how to get
legislation passed, but not in the world that policy acts upon.
Perhaps no one could be.[3]
This sense of "paradigm" was defined by Thomas Kuhn in his
Structure of Scientific Revolutions, but I also recommend his
Copernican Revolution, where you can see him at work developing the
idea.[4]
This is one reason people with a touch of Asperger's may have
an advantage in discovering new ideas. They're always flying on
instruments.[5]
Hall, Rupert. From Galileo to Newton. Collins, 1963. This
book is particularly good at getting into contemporaries' heads.Thanks to Trevor Blackwell, Patrick Collison, Suhail Doshi, Daniel
Gackle, Jessica Livingston, and Robert Morris for reading drafts of this.November 2005In the next few years, venture capital funds will find themselves
squeezed from four directions.  They're already stuck with a seller's
market, because of the huge amounts they raised at the end of the
Bubble and still haven't invested.  This by itself is not the end
of the world.  In fact, it's just a more extreme version of the
norm
in the VC business: too much money chasing too few deals.Unfortunately, those few deals now want less and less money, because
it's getting so cheap to start a startup.  The four causes: open
source, which makes software free; Moore's law, which makes hardware
geometrically closer to free; the Web, which makes promotion free
if you're good; and better languages, which make development a lot
cheaper.When we started our startup in 1995, the first three were our biggest
expenses.  We had to pay $5000 for the Netscape Commerce Server,
the only software that then supported secure http connections.  We
paid $3000 for a server with a 90 MHz processor and 32 meg of
memory.  And we paid a PR firm about $30,000 to promote our launch.Now you could get all three for nothing.  You can get the software
for free; people throw away computers more powerful than our first
server; and if you make something good you can generate ten times
as much traffic by word of mouth online than our first PR firm got
through the print media.And of course another big change for the average startup is that
programming languages have improved-- or rather, the median language has.  At most startups ten years
ago, software development meant ten programmers writing code in
C++.  Now the same work might be done by one or two using Python
or Ruby.During the Bubble, a lot of people predicted that startups would
outsource their development to India.  I think a better model for
the future is David Heinemeier Hansson, who outsourced his development
to a more powerful language instead.  A lot of well-known applications
are now, like BaseCamp, written by just one programmer.  And one
guy is more than 10x cheaper than ten, because (a) he won't waste
any time in meetings, and (b) since he's probably a founder, he can
pay himself nothing.Because starting a startup is so cheap, venture capitalists now
often want to give startups more money than the startups want to
take.  VCs like to invest several million at a time.  But as one
VC told me after a startup he funded would only take about half a
million, "I don't know what we're going to do.  Maybe we'll just
have to give some of it back." Meaning give some of the fund back
to the institutional investors who supplied it, because it wasn't
going to be possible to invest it all.Into this already bad situation comes the third problem: Sarbanes-Oxley.
Sarbanes-Oxley is a law, passed after the Bubble, that drastically
increases the regulatory burden on public companies. And in addition
to the cost of compliance, which is at least two million dollars a
year, the law introduces frightening legal exposure for corporate
officers.  An experienced CFO I know said flatly: "I would not
want to be CFO of a public company now."You might think that responsible corporate governance is an area
where you can't go too far.  But you can go too far in any law, and
this remark convinced me that Sarbanes-Oxley must have.  This CFO
is both the smartest and the most upstanding money guy I know.  If
Sarbanes-Oxley deters people like him from being CFOs of public  
companies, that's proof enough that it's broken.Largely because of Sarbanes-Oxley, few startups go public now.  For
all practical purposes, succeeding now equals getting bought.  Which
means VCs are now in the business of finding promising little 2-3
man startups and pumping them up into companies that cost $100
million to acquire.   They didn't mean to be in this business; it's
just what their business has evolved into.Hence the fourth problem: the acquirers have begun to realize they
can buy wholesale.  Why should they wait for VCs to make the startups
they want more expensive?  Most of what the VCs add, acquirers don't
want anyway.  The acquirers already have brand recognition and HR
departments.  What they really want is the software and the developers,
and that's what the startup is in the early phase: concentrated
software and developers.Google, typically, seems to have been the first to figure this out.
"Bring us your startups early," said Google's speaker at the Startup School.  They're quite
explicit about it: they like to acquire startups at just the point
where they would do a Series A round.  (The Series A round is the
first round of real VC funding; it usually happens in the first
year.) It is a brilliant strategy, and one that other big technology
companies will no doubt try to duplicate.  Unless they want to have 
still more of their lunch eaten by Google.Of course, Google has an advantage in buying startups: a lot of the
people there are rich, or expect to be when their options vest.
Ordinary employees find it very hard to recommend an acquisition;
it's just too annoying to see a bunch of twenty year olds get rich
when you're still working for salary.  Even if it's the right thing   
for your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.
They need to do two things, one of which won't surprise them, and  
another that will seem an anathema.Let's start with the obvious one: lobby to get Sarbanes-Oxley  
loosened.  This law was created to prevent future Enrons, not to
destroy the IPO market.  Since the IPO market was practically dead
when it passed, few saw what bad effects it would have.  But now 
that technology has recovered from the last bust, we can see clearly
what a bottleneck Sarbanes-Oxley has become.Startups are fragile plants—seedlings, in fact.  These seedlings
are worth protecting, because they grow into the trees of the
economy.  Much of the economy's growth is their growth.  I think
most politicians realize that.  But they don't realize just how   
fragile startups are, and how easily they can become collateral
damage of laws meant to fix some other problem.Still more dangerously, when you destroy startups, they make very
little noise.  If you step on the toes of the coal industry, you'll
hear about it.  But if you inadvertantly squash the startup industry,
all that happens is that the founders of the next Google stay in 
grad school instead of starting a company.My second suggestion will seem shocking to VCs: let founders cash  
out partially in the Series A round.  At the moment, when VCs invest
in a startup, all the stock they get is newly issued and all the 
money goes to the company.  They could buy some stock directly from
the founders as well.Most VCs have an almost religious rule against doing this.  They
don't want founders to get a penny till the company is sold or goes
public.  VCs are obsessed with control, and they worry that they'll
have less leverage over the founders if the founders have any money.This is a dumb plan.  In fact, letting the founders sell a little stock
early would generally be better for the company, because it would
cause the founders' attitudes toward risk to be aligned with the
VCs'.  As things currently work, their attitudes toward risk tend
to be diametrically opposed: the founders, who have nothing, would
prefer a 100% chance of $1 million to a 20% chance of $10 million,
while the VCs can afford to be "rational" and prefer the latter.Whatever they say, the reason founders are selling their companies
early instead of doing Series A rounds is that they get paid up
front.  That first million is just worth so much more than the
subsequent ones.  If founders could sell a little stock early,
they'd be happy to take VC money and bet the rest on a bigger
outcome.So why not let the founders have that first million, or at least
half million?  The VCs would get same number of shares for the   
money.  So what if some of the money would go to the  
founders instead of the company?Some VCs will say this is
unthinkable—that they want all their money to be put to work
growing the company.  But the fact is, the huge size of current VC
investments is dictated by the structure
of VC funds, not the needs of startups.  Often as not these large  
investments go to work destroying the company rather than growing
it.The angel investors who funded our startup let the founders sell
some stock directly to them, and it was a good deal for everyone. 
The angels made a huge return on that investment, so they're happy.
And for us founders it blunted the terrifying all-or-nothingness
of a startup, which in its raw form is more a distraction than a
motivator.If VCs are frightened at the idea of letting founders partially
cash out, let me tell them something still more frightening: you
are now competing directly with Google.
Thanks to Trevor Blackwell, Sarah Harlin, Jessica
Livingston, and Robert Morris for reading drafts of this.May 2007People who worry about the increasing gap between rich and poor
generally look back on the mid twentieth century as a golden age.
In those days we had a large number of high-paying union manufacturing
jobs that boosted the median income.  I wouldn't quite call the
high-paying union job a myth, but I think people who dwell on it
are reading too much into it.Oddly enough, it was working with startups that made me realize
where the high-paying union job came from.  In a rapidly growing
market, you don't worry too much about efficiency.  It's more
important to grow fast.  If there's some mundane problem getting
in your way, and there's a simple solution that's somewhat expensive,
just take it and get on with more important things.  EBay didn't
win by paying less for servers than their competitors.Difficult though it may be to imagine now, manufacturing was a
growth industry in the mid twentieth century.  This was an era when
small firms making everything from cars to candy were getting
consolidated into a new kind of corporation with national reach and
huge economies of scale.  You had to grow fast or die.  Workers
were for these companies what servers are for an Internet startup.
A reliable supply was more important than low cost.If you looked in the head of a 1950s auto executive, the attitude
must have been: sure, give 'em whatever they ask for, so long as
the new model isn't delayed.In other words, those workers were not paid what their work was
worth.  Circumstances being what they were, companies would have
been stupid to insist on paying them so little.If you want a less controversial example of this phenomenon, ask
anyone who worked as a consultant building web sites during the
Internet Bubble.  In the late nineties you could get paid huge sums
of money for building the most trivial things.  And yet does anyone
who was there have any expectation those days will ever return?  I
doubt it.  Surely everyone realizes that was just a temporary
aberration.The era of labor unions seems to have been the same kind of aberration, 
just spread
over a longer period, and mixed together with a lot of ideology
that prevents people from viewing it with as cold an eye as they
would something like consulting during the Bubble.Basically, unions were just Razorfish.People who think the labor movement was the creation of heroic union
organizers have a problem to explain: why are unions shrinking now?
The best they can do is fall back on the default explanation of
people living in fallen civilizations.  Our ancestors were giants.
The workers of the early twentieth century must have had a moral
courage that's lacking today.In fact there's a simpler explanation.  The early twentieth century
was just a fast-growing startup overpaying for infrastructure.  And
we in the present are not a fallen people, who have abandoned
whatever mysterious high-minded principles produced the high-paying
union job.  We simply live in a time when the fast-growing companies
overspend on different things.January 2015Corporate Development, aka corp dev, is the group within companies
that buys other companies. If you're talking to someone from corp
dev, that's why, whether you realize it yet or not.It's usually a mistake to talk to corp dev unless (a) you want to
sell your company right now and (b) you're sufficiently likely to
get an offer at an acceptable price.  In practice that means startups
should only talk to corp dev when they're either doing really well
or really badly.  If you're doing really badly, meaning the company
is about to die, you may as well talk to them, because you have
nothing to lose. And if you're doing really well, you can safely
talk to them, because you both know the price will have to be high,
and if they show the slightest sign of wasting your time, you'll
be confident enough to tell them to get lost.The danger is to companies in the middle.  Particularly to young
companies that are growing fast, but haven't been doing it for long
enough to have grown big yet.  It's usually a mistake for a promising
company less than a year old even to talk to corp dev.But it's a mistake founders constantly make.  When someone from
corp dev wants to meet, the founders tell themselves they should
at least find out what they want.  Besides, they don't want to
offend Big Company by refusing to meet.Well, I'll tell you what they want.  They want to talk about buying
you.  That's what the title "corp dev" means.   So before agreeing
to meet with someone from corp dev, ask yourselves, "Do we want to
sell the company right now?"  And if the answer is no, tell them
"Sorry, but we're focusing on growing the company."  They won't be
offended.  And certainly the founders of Big Company won't be
offended. If anything they'll think more highly of you.  You'll
remind them of themselves.  They didn't sell either; that's why
they're in a position now to buy other companies.
[1]Most founders who get contacted by corp dev already know what it
means.  And yet even when they know what corp dev does and know
they don't want to sell, they take the meeting.  Why do they do it?
The same mix of denial and wishful thinking that underlies most
mistakes founders make. It's flattering to talk to someone who wants
to buy you.  And who knows, maybe their offer will be surprisingly
high.  You should at least see what it is, right?No.  If they were going to send you an offer immediately by email,
sure, you might as well open it.  But that is not how conversations
with corp dev work.  If you get an offer at all, it will be at the
end of a long and unbelievably distracting process.  And if the
offer is surprising, it will be surprisingly low.Distractions are the thing you can least afford in a startup.  And
conversations with corp dev are the worst sort of distraction,
because as well as consuming your attention they undermine your
morale.  One of the tricks to surviving a grueling process is not
to stop and think how tired you are.  Instead you get into a sort
of flow. 
[2]
Imagine what it would do to you if at mile 20 of a
marathon, someone ran up beside you and said "You must feel really
tired.  Would you like to stop and take a rest?"  Conversations
with corp dev are like that but worse, because the suggestion of
stopping gets combined in your mind with the imaginary high price
you think they'll offer.And then you're really in trouble.  If they can, corp dev people
like to turn the tables on you. They like to get you to the point
where you're trying to convince them to buy instead of them trying
to convince you to sell.  And surprisingly often they succeed.This is a very slippery slope, greased with some of the most powerful
forces that can work on founders' minds, and attended by an experienced
professional whose full time job is to push you down it.Their tactics in pushing you down that slope are usually fairly
brutal. Corp dev people's whole job is to buy companies, and they
don't even get to choose which.  The only way their performance is
measured is by how cheaply they can buy you, and the more ambitious
ones will stop at nothing to achieve that. For example, they'll
almost always start with a lowball offer, just to see if you'll
take it. Even if you don't, a low initial offer will demoralize you
and make you easier to manipulate.And that is the most innocent of their tactics. Just wait till
you've agreed on a price and think you have a done deal, and then
they come back and say their boss has vetoed the deal and won't do
it for more than half the agreed upon price. Happens all the time.
If you think investors can behave badly, it's nothing compared to
what corp dev people can do.  Even corp dev people at companies
that are otherwise benevolent.I remember once complaining to a
friend at Google about some nasty trick their corp dev people had
pulled on a YC startup."What happened to Don't be Evil?" I asked."I don't think corp dev got the memo," he replied.The tactics you encounter in M&A conversations can be like nothing
you've experienced in the otherwise comparatively 
upstanding world
of Silicon Valley.  It's as if a chunk of genetic material from the
old-fashioned robber baron business world got incorporated into the
startup world.
[3]The simplest way to protect yourself is to use the trick that John
D. Rockefeller, whose grandfather was an alcoholic, used to protect
himself from becoming one.  He once told a Sunday school class

  Boys, do you know why I never became a drunkard?  Because I never
  took the first drink.

Do you want to sell your company right now?  Not eventually, right
now.  If not, just don't take the first meeting.  They won't be
offended.  And you in turn will be guaranteed to be spared one of
the worst experiences that can happen to a startup.If you do want to sell, there's another set of 
techniques
 for doing
that.  But the biggest mistake founders make in dealing with corp
dev is not doing a bad job of talking to them when they're ready
to, but talking to them before they are.  So if you remember only
the title of this essay, you already know most of what you need to
know about M&A in the first year.Notes[1]
I'm not saying you should never sell.  I'm saying you should
be clear in your own mind about whether you want to sell or not,
and not be led by manipulation or wishful thinking into trying to
sell earlier than you otherwise would have.[2]
In a startup, as in most competitive sports, the task at hand
almost does this for you; you're too busy to feel tired.  But when
you lose that protection, e.g. at the final whistle, the fatigue
hits you like a wave.  To talk to corp dev is to let yourself feel
it mid-game.[3]
To be fair, the apparent misdeeds of corp dev people are magnified
by the fact that they function as the face of a large organization
that often doesn't know its own mind.  Acquirers can be surprisingly
indecisive about acquisitions, and their flakiness is indistinguishable
from dishonesty by the time it filters down to you.Thanks to Marc Andreessen, Jessica Livingston, Geoff
Ralston, and Qasar Younis for reading drafts of this.April 2004To the popular press, "hacker" means someone who breaks
into computers.  Among programmers it means a good programmer.
But the two meanings are connected.  To programmers,
"hacker" connotes mastery in the most literal sense: someone
who can make a computer do what he wants—whether the computer
wants to or not.To add to the confusion, the noun "hack" also has two senses.  It can
be either a compliment or an insult.  It's called a hack when
you do something in an ugly way.  But when you do something
so clever that you somehow beat the system, that's also
called a hack.  The word is used more often in the former than
the latter sense, probably because ugly solutions are more
common than brilliant ones.Believe it or not, the two senses of "hack" are also
connected.  Ugly and imaginative solutions have something in
common: they both break the rules.  And there is a gradual
continuum between rule breaking that's merely ugly (using
duct tape to attach something to your bike) and rule breaking
that is brilliantly imaginative (discarding Euclidean space).Hacking predates computers.  When he
was working on the Manhattan Project, Richard Feynman used to
amuse himself by breaking into safes containing secret documents.
This tradition continues today.
When we were in grad school, a hacker friend of mine who spent too much
time around MIT had
his own lock picking kit.
(He now runs a hedge fund, a not unrelated enterprise.)It is sometimes hard to explain to authorities why one would
want to do such things.
Another friend of mine once got in trouble with the government for
breaking into computers.  This had only recently been declared
a crime, and the FBI found that their usual investigative
technique didn't work.  Police investigation apparently begins with
a motive.  The usual motives are few: drugs, money, sex,
revenge.  Intellectual curiosity was not one of the motives on
the FBI's list.  Indeed, the whole concept seemed foreign to
them.Those in authority tend to be annoyed by hackers'
general attitude of disobedience.  But that disobedience is
a byproduct of the qualities that make them good programmers.
They may laugh at the CEO when he talks in generic corporate
newspeech, but they also laugh at someone who tells them
a certain problem can't be solved.
Suppress one, and you suppress the other.This attitude is sometimes affected.  Sometimes young programmers
notice the eccentricities of eminent hackers and decide to
adopt some of their own in order to seem smarter.
The fake version is not merely
annoying; the prickly attitude of these posers
can actually slow the process of innovation.But even factoring in their annoying eccentricities,
the disobedient attitude of hackers is a net win.  I wish its
advantages were better understood.For example, I suspect people in Hollywood are
simply mystified by
hackers' attitudes toward copyrights.  They are a perennial
topic of heated discussion on Slashdot.
But why should people who program computers
be so concerned about copyrights, of all things?Partly because some companies use mechanisms to prevent
copying.  Show any hacker a lock and his first thought is
how to pick it.  But there is a deeper reason that
hackers are alarmed by measures like copyrights and patents.
They see increasingly aggressive measures to protect
"intellectual property"
as a threat to the intellectual
freedom they need to do their job.
And they are right.It is by poking about inside current technology that
hackers get ideas for the next generation.  No thanks,
intellectual homeowners may say, we don't need any
outside help.  But they're wrong.
The next generation of computer technology has
often—perhaps more often than not—been developed by outsiders.In 1977 there was no doubt some group within IBM developing
what they expected to be
the next generation of business computer.  They were mistaken.
The next generation of business computer was
being developed on entirely different lines by two long-haired
guys called Steve in a garage in Los Altos.  At about the
same time, the powers that be
were cooperating to develop the
official next generation operating system, Multics.
But two guys who thought Multics excessively complex went off
and wrote their own.  They gave it a name that
was a joking reference to Multics: Unix.The latest intellectual property laws impose
unprecedented restrictions on the sort of poking around that
leads to new ideas. In the past, a competitor might use patents
to prevent you from selling a copy of something they
made, but they couldn't prevent you from
taking one apart to see how it worked.   The latest
laws make this a crime.  How are we
to develop new technology if we can't study current
technology to figure out how to improve it?Ironically, hackers have brought this on themselves.
Computers are responsible for the problem.  The control systems
inside machines used to be physical: gears and levers and cams.
Increasingly, the brains (and thus the value) of products is
in software. And by this I mean software in the general sense:
i.e. data.  A song on an LP is physically stamped into the
plastic.  A song on an iPod's disk is merely stored on it.Data is by definition easy to copy.  And the Internet
makes copies easy to distribute.  So it is no wonder
companies are afraid.  But, as so often happens, fear has
clouded their judgement.  The government has responded
with draconian laws to protect intellectual property.
They probably mean well. But
they may not realize that such laws will do more harm
than good.Why are programmers so violently opposed to these laws?
If I were a legislator, I'd be interested in this
mystery—for the same reason that, if I were a farmer and suddenly
heard a lot of squawking coming from my hen house one night,
I'd want to go out and investigate.  Hackers are not stupid,
and unanimity is very rare in this world.
So if they're all squawking,   
perhaps there is something amiss.Could it be that such laws, though intended to protect America,
will actually harm it?  Think about it.  There is something
very American about Feynman breaking into safes during
the Manhattan Project.  It's hard to imagine the authorities
having a sense of humor about such things over
in Germany at that time.  Maybe it's not a coincidence.Hackers are unruly.  That is the essence of hacking.  And it
is also the essence of Americanness.  It is no accident
that Silicon Valley
is in America, and not France, or Germany,
or England, or Japan. In those countries, people color inside
the lines.I lived for a while in Florence.  But after I'd been there
a few months I realized that what I'd been unconsciously hoping
to find there was back in the place I'd just left.
The reason Florence is famous is that in 1450, it was New York.
In 1450 it was filled with the kind of turbulent and ambitious
people you find now in America.  (So I went back to America.)It is greatly to America's advantage that it is
a congenial atmosphere for the right sort of unruliness—that
it is a home not just for the smart, but for smart-alecks.
And hackers are invariably smart-alecks.  If we had a national
holiday, it would be April 1st.  It says a great deal about
our work that we use the same word for a brilliant or a
horribly cheesy solution.   When we cook one up we're not
always 100% sure which kind it is.  But as long as it has
the right sort of wrongness, that's a promising sign.
It's odd that people
think of programming as precise and methodical.  Computers
are precise and methodical.  Hacking is something you do
with a gleeful laugh.In our world some of the most characteristic solutions
are not far removed from practical
jokes.  IBM was no doubt rather surprised by the consequences
of the licensing deal for DOS, just as the hypothetical
"adversary" must be when Michael Rabin solves a problem by
redefining it as one that's easier to solve.Smart-alecks have to develop a keen sense of how much they
can get away with.  And lately hackers 
have sensed a change
in the atmosphere.
Lately hackerliness seems rather frowned upon.To hackers the recent contraction in civil liberties seems
especially ominous.  That must also mystify outsiders. 
Why should we care especially about civil
liberties?  Why programmers, more than
dentists or salesmen or landscapers?Let me put the case in terms a government official would appreciate.
Civil liberties are not just an ornament, or a quaint
American tradition.  Civil liberties make countries rich.
If you made a graph of
GNP per capita vs. civil liberties, you'd notice a definite
trend.  Could civil liberties really be a cause, rather
than just an effect?  I think so.  I think a society in which
people can do and say what they want will also tend to
be one in which the most efficient solutions win, rather than
those sponsored by the most influential people.
Authoritarian countries become corrupt;
corrupt countries become poor; and poor countries are weak. 
It seems to me there is
a Laffer curve for government power, just as for
tax revenues.  At least, it seems likely enough that it
would be stupid to try the experiment and find out.  Unlike
high tax rates, you can't repeal totalitarianism if it
turns out to be a mistake.This is why hackers worry.  The government spying on people doesn't
literally make programmers write worse code.  It just leads
eventually to a world in which bad ideas win.  And because
this is so important to hackers, they're especially sensitive
to it.  They can sense totalitarianism approaching from a
distance, as animals can sense an approaching  
thunderstorm.It would be ironic if, as hackers fear, recent measures
intended to protect national security and intellectual property
turned out to be a missile aimed right at what makes   
America successful.  But it would not be the first time that
measures taken in an atmosphere of panic had
the opposite of the intended effect.There is such a thing as Americanness.
There's nothing like living abroad to teach you that.   
And if you want to know whether something will nurture or squash
this quality, it would be hard to find a better focus
group than hackers, because they come closest of any group
I know to embodying it.  Closer, probably,  than
the men running our government,
who for all their talk of patriotism
remind me more of Richelieu or Mazarin
than Thomas Jefferson or George Washington.When you read what the founding fathers had to say for
themselves, they sound more like hackers.
"The spirit of resistance to government,"
Jefferson wrote, "is so valuable on certain occasions, that I wish
it always to be kept alive."Imagine an American president saying that today.
Like the remarks of an outspoken old grandmother, the sayings of
the founding fathers have embarrassed generations of
their less confident successors.  They remind us where we come from.
They remind us that it is the people who break rules that are
the source of America's wealth and power.Those in a position to impose rules naturally want them to be
obeyed.  But be careful what you ask for. You might get it.Thanks to Ken Anderson, Trevor Blackwell, Daniel Giffin, 
Sarah Harlin,  Shiro Kawai, Jessica Livingston, Matz, 
Jackie McDonough, Robert Morris, Eric Raymond, Guido van Rossum,
David Weinberger, and
Steven Wolfram for reading drafts of this essay.
(The image shows Steves Jobs and Wozniak 
with a "blue box."
Photo by Margret Wozniak. Reproduced by permission of Steve
Wozniak.)November 2022Since I was about 9 I've been puzzled by the apparent contradiction
between being made of matter that behaves in a predictable way, and
the feeling that I could choose to do whatever I wanted. At the
time I had a self-interested motive for exploring the question. At
that age (like most succeeding ages) I was always in trouble with
the authorities, and it seemed to me that there might possibly be
some way to get out of trouble by arguing that I wasn't responsible
for my actions. I gradually lost hope of that, but the puzzle
remained: How do you reconcile being a machine made of matter with
the feeling that you're free to choose what you do?
[1]The best way to explain the answer may be to start with a slightly
wrong version, and then fix it. The wrong version is: You can do
what you want, but you can't want what you want. Yes, you can control
what you do, but you'll do what you want, and you can't control
that.The reason this is mistaken is that people do sometimes change what
they want. People who don't want to want something — drug addicts,
for example — can sometimes make themselves stop wanting it. And
people who want to want something — who want to like classical
music, or broccoli — sometimes succeed.So we modify our initial statement: You can do what you want, but
you can't want to want what you want.That's still not quite true. It's possible to change what you want
to want. I can imagine someone saying "I decided to stop wanting
to like classical music." But we're getting closer to the truth.
It's rare for people to change what they want to want, and the more
"want to"s we add, the rarer it gets.We can get arbitrarily close to a true statement by adding more "want
to"s in much the same way we can get arbitrarily close to 1 by adding
more 9s to a string of 9s following a decimal point. In practice
three or four "want to"s must surely be enough. It's hard even to
envision what it would mean to change what you want to want to want
to want, let alone actually do it.So one way to express the correct answer is to use a regular
expression. You can do what you want, but there's some statement
of the form "you can't (want to)* want what you want" that's true.
Ultimately you get back to a want that you don't control.
[2]
Notes[1]
I didn't know when I was 9 that matter might behave randomly,
but I don't think it affects the problem much. Randomness destroys
the ghost in the machine as effectively as determinism.[2]
If you don't like using an expression, you can make the same
point using higher-order desires: There is some n such that you
don't control your nth-order desires.
Thanks to Trevor Blackwell,
Jessica Livingston, Robert Morris, and
Michael Nielsen for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




January 2006To do something well you have to like it.   That idea is not exactly
novel.  We've got it down to four words: "Do what you love."  But
it's not enough just to tell people that.  Doing what you love is
complicated.The very idea is foreign to what most of us learn as kids.  When I
was a kid, it seemed as if work and fun were opposites by definition.
Life had two states: some of the time adults were making you do
things, and that was called work; the rest of the time you could
do what you wanted, and that was called playing.  Occasionally the
things adults made you do were fun, just as, occasionally, playing
wasn't—for example, if you fell and hurt yourself.  But except
for these few anomalous cases, work was pretty much defined as
not-fun.And it did not seem to be an accident. School, it was implied, was
tedious because it was preparation for grownup work.The world then was divided into two groups, grownups and kids.
Grownups, like some kind of cursed race, had to work.  Kids didn't,
but they did have to go to school, which was a dilute version of
work meant to prepare us for the real thing.  Much as we disliked
school, the grownups all agreed that grownup work was worse, and
that we had it easy.Teachers in particular all seemed to believe implicitly that work
was not fun.  Which is not surprising: work wasn't fun for most of
them.  Why did we have to memorize state capitals instead of playing
dodgeball?  For the same reason they had to watch over a bunch of
kids instead of lying on a beach.  You couldn't just do what you
wanted.I'm not saying we should let little kids do whatever they want.
They may have to be made to work on certain things.  But if we make
kids work on dull stuff, it might be wise to tell them that tediousness
is not the defining quality of work, and indeed that the reason
they have to work on dull stuff now is so they can work on more
interesting stuff later.
[1]Once, when I was about 9 or 10, my father told me I could be whatever
I wanted when I grew up, so long as I enjoyed it.  I remember that
precisely because it seemed so anomalous.  It was like being told
to use dry water.  Whatever I thought he meant, I didn't think he
meant work could literally be fun—fun like playing.  It
took me years to grasp that.JobsBy high school, the prospect of an actual job was on the horizon.
Adults would sometimes come to speak to us about their work, or we
would go to see them at work.  It was always understood that they
enjoyed what they did.  In retrospect I think one may have: the
private jet pilot.  But I don't think the bank manager really did.The main reason they all acted as if they enjoyed their work was
presumably the upper-middle class convention that you're supposed
to.  It would not merely be bad for your career to say that you
despised your job, but a social faux-pas.Why is it conventional to pretend to like what you do?  The first
sentence of this essay explains that.  If you have to like something
to do it well, then the most successful people will all like what
they do.  That's where the upper-middle class tradition comes from.
Just as houses all over America are full of 
chairs
that are, without
the owners even knowing it, nth-degree imitations of chairs designed
250 years ago for French kings, conventional attitudes about work
are, without the owners even knowing it, nth-degree imitations of
the attitudes of people who've done great things.What a recipe for alienation.  By the time they reach an age to
think about what they'd like to do, most kids have been thoroughly
misled about the idea of loving one's work.  School has trained
them to regard work as an unpleasant duty.  Having a job is said
to be even more onerous than schoolwork.  And yet all the adults
claim to like what they do.  You can't blame kids for thinking "I
am not like these people; I am not suited to this world."Actually they've been told three lies: the stuff they've been taught
to regard as work in school is not real work; grownup work is not
(necessarily) worse than schoolwork; and many of the adults around
them are lying when they say they like what they do.The most dangerous liars can be the kids' own parents.  If you take
a boring job to give your family a high standard of living, as so
many people do, you risk infecting your kids with the idea that
work is boring. 
[2]
Maybe it would be better for kids in this one
case if parents were not so unselfish.  A parent who set an example
of loving their work might help their kids more than an expensive
house.
[3]It was not till I was in college that the idea of work finally broke
free from the idea of making a living.  Then the important question
became not how to make money, but what to work on.  Ideally these
coincided, but some spectacular boundary cases (like Einstein in
the patent office) proved they weren't identical.The definition of work was now to make some original contribution
to the world, and in the process not to starve.  But after the habit
of so many years my idea of work still included a large component
of pain.  Work still seemed to require discipline, because only
hard problems yielded grand results, and hard problems couldn't
literally be fun.   Surely one had to force oneself to work on them.If you think something's supposed to hurt, you're less likely to
notice if you're doing it wrong.  That about sums up my experience
of graduate school.BoundsHow much are you supposed to like what you do?  Unless you
know that, you don't know when to stop searching. And if, like most
people, you underestimate it, you'll tend to stop searching too
early.  You'll end up doing something chosen for you by your parents,
or the desire to make money, or prestige—or sheer inertia.Here's an upper bound: Do what you love doesn't mean, do what you
would like to do most this second.  Even Einstein probably
had moments when he wanted to have a cup of coffee, but told himself
he ought to finish what he was working on first.It used to perplex me when I read about people who liked what they
did so much that there was nothing they'd rather do.  There didn't
seem to be any sort of work I liked that much.  If I had a
choice of (a) spending the next hour working on something or (b)
be teleported to Rome and spend the next hour wandering about, was
there any sort of work I'd prefer?  Honestly, no.But the fact is, almost anyone would rather, at any given moment,
float about in the Carribbean, or have sex, or eat some delicious
food, than work on hard problems.  The rule about doing what you
love assumes a certain length of time.  It doesn't mean, do what
will make you happiest this second, but what will make you happiest
over some longer period, like a week or a month.Unproductive pleasures pall eventually.  After a while you get tired
of lying on the beach.  If you want to stay happy, you have to do
something.As a lower bound, you have to like your work more than any unproductive
pleasure.  You have to like what you do enough that the concept of
"spare time" seems mistaken.  Which is not to say you have to spend
all your time working.  You can only work so much before you get
tired and start to screw up.  Then you want to do something else—even something mindless.  But you don't regard this time as the
prize and the time you spend working as the pain you endure to earn
it.I put the lower bound there for practical reasons.  If your work
is not your favorite thing to do, you'll have terrible problems
with procrastination.  You'll have to force yourself to work,  and
when you resort to that the results are distinctly inferior.To be happy I think you have to be doing something you not only
enjoy, but admire.  You have to be able to say, at the end, wow,
that's pretty cool.  This doesn't mean you have to make something.
If you learn how to hang glide, or to speak a foreign language
fluently, that will be enough to make you say, for a while at least,
wow, that's pretty cool.  What there has to be is a test.So one thing that falls just short of the standard, I think, is
reading books.  Except for some books in math and the hard sciences,
there's no test of how well you've read a book, and that's why
merely reading books doesn't quite feel like work.  You have to do
something with what you've read to feel productive.I think the best test is one Gino Lee taught me: to try to do things
that would make your friends say wow.  But it probably wouldn't
start to work properly till about age 22, because most people haven't
had a big enough sample to pick friends from before then.SirensWhat you should not do, I think, is worry about the opinion of
anyone beyond your friends.  You shouldn't worry about prestige.
Prestige is the opinion of the rest of the world.  When you can ask
the opinions of people whose judgement you respect, what does it
add to consider the opinions of people you don't even know? 
[4]This is easy advice to give.  It's hard to follow, especially when
you're young.  
[5]
Prestige is like a powerful magnet that warps
even your beliefs about what you enjoy.  It causes you to work not
on what you like, but what you'd like to like.That's what leads people to try to write novels, for example.  They
like reading novels.  They notice that people who write them win
Nobel prizes.  What could be more wonderful, they think, than to
be a novelist?  But liking the idea of being a novelist is not
enough; you have to like the actual work of novel-writing if you're
going to be good at it; you have to like making up elaborate lies.Prestige is just fossilized inspiration.  If you do anything well
enough, you'll make it prestigious.  Plenty of things we now
consider prestigious were anything but at first.  Jazz comes to
mind—though almost any established art form would do.   So just
do what you like, and let prestige take care of itself.Prestige is especially dangerous to the ambitious.  If you want to
make ambitious people waste their time on errands, the way to do
it is to bait the hook with prestige.  That's the recipe for getting
people to give talks, write forewords, serve on committees, be
department heads, and so on.  It might be a good rule simply to
avoid any prestigious task. If it didn't suck, they wouldn't have
had to make it prestigious.Similarly, if you admire two kinds of work equally, but one is more
prestigious, you should probably choose the other.  Your opinions
about what's admirable are always going to be slightly influenced
by prestige, so if the two seem equal to you, you probably have
more genuine admiration for the less prestigious one.The other big force leading people astray is money.  Money by itself
is not that dangerous.  When something pays well but is regarded
with contempt, like telemarketing, or prostitution, or personal
injury litigation, ambitious people aren't tempted by it.  That
kind of work ends up being done by people who are "just trying to
make a living."  (Tip: avoid any field whose practitioners say
this.)  The danger is when money is combined with prestige, as in,
say, corporate law, or medicine.  A comparatively safe and prosperous
career with some automatic baseline prestige is dangerously tempting
to someone young, who hasn't thought much about what they really
like.The test of whether people love what they do is whether they'd do
it even if they weren't paid for it—even if they had to work at
another job to make a living.  How many corporate lawyers would do
their current work if they had to do it for free, in their spare
time, and take day jobs as waiters to support themselves?This test is especially helpful in deciding between different kinds
of academic work, because fields vary greatly in this respect.  Most
good mathematicians would work on math even if there were no jobs
as math professors, whereas in the departments at the other end of
the spectrum, the availability of teaching jobs is the driver:
people would rather be English professors than work in ad agencies,
and publishing papers is the way you compete for such jobs.  Math
would happen without math departments, but it is the existence of
English majors, and therefore jobs teaching them, that calls into
being all those thousands of dreary papers about gender and identity
in the novels of Conrad.  No one does 
that 
kind of thing for fun.The advice of parents will tend to err on the side of money.  It
seems safe to say there are more undergrads who want to be novelists
and whose parents want them to be doctors than who want to be doctors
and whose parents want them to be novelists.  The kids think their
parents are "materialistic." Not necessarily.  All parents tend to
be more conservative for their kids than they would for themselves,
simply because, as parents, they share risks more than rewards.  If
your eight year old son decides to climb a tall tree, or your teenage
daughter decides to date the local bad boy, you won't get a share
in the excitement, but if your son falls, or your daughter gets
pregnant, you'll have to deal with the consequences.DisciplineWith such powerful forces leading us astray, it's not surprising
we find it so hard to discover what we like to work on.  Most people
are doomed in childhood by accepting the axiom that work = pain.
Those who escape this are nearly all lured onto the rocks by prestige
or money.  How many even discover something they love to work on?
A few hundred thousand, perhaps, out of billions.It's hard to find work you love; it must be, if so few do.  So don't
underestimate this task.  And don't feel bad if you haven't succeeded
yet.  In fact, if you admit to yourself that you're discontented,
you're a step ahead of most people, who are still in denial.  If
you're surrounded by colleagues who claim to enjoy work that you
find contemptible, odds are they're lying to themselves.  Not
necessarily, but probably.Although doing great work takes less discipline than people think—because the way to do great work is to find something you like so
much that you don't have to force yourself to do it—finding
work you love does usually require discipline.   Some people are
lucky enough to know what they want to do when they're 12, and just
glide along as if they were on railroad tracks.  But this seems the
exception.  More often people who do great things have careers with
the trajectory of a ping-pong ball.  They go to school to study A,
drop out and get a job doing B, and then become famous for C after
taking it up on the side.Sometimes jumping from one sort of work to another is a sign of
energy, and sometimes it's a sign of laziness.  Are you dropping
out, or boldly carving a new path?  You often can't tell yourself.
Plenty of people who will later do great things seem to be disappointments
early on, when they're trying to find their niche.Is there some test you can use to keep yourself honest?  One is to
try to do a good job at whatever you're doing, even if you don't
like it.  Then at least you'll know you're not using dissatisfaction
as an excuse for being lazy.  Perhaps more importantly, you'll get
into the habit of doing things well.Another test you can use is: always produce.  For example, if you
have a day job you don't take seriously because you plan to be a
novelist, are you producing?  Are you writing pages of fiction,
however bad?  As long as you're producing, you'll know you're not
merely using the hazy vision of the grand novel you plan to write
one day as an opiate.  The view of it will be obstructed by the all
too palpably flawed one you're actually writing."Always produce" is also a heuristic for finding the work you love.
If you subject yourself to that constraint, it will automatically
push you away from things you think you're supposed to work on,
toward things you actually like.  "Always produce" will discover
your life's work the way water, with the aid of gravity, finds the
hole in your roof.Of course, figuring out what you like to work on doesn't mean you
get to work on it.  That's a separate question.  And if you're
ambitious you have to keep them separate: you have to make a conscious
effort to keep your ideas about what you want from being contaminated
by what seems possible. 
[6]It's painful to keep them apart, because it's painful to observe
the gap between them. So most people pre-emptively lower their
expectations.  For example, if you asked random people on the street
if they'd like to be able to draw like Leonardo, you'd find most
would say something like "Oh, I can't draw."  This is more a statement
of intention than fact; it means, I'm not going to try.  Because
the fact is, if you took a random person off the street and somehow
got them to work as hard as they possibly could at drawing for the
next twenty years, they'd get surprisingly far.  But it would require
a great moral effort; it would mean staring failure in the eye every
day for years.  And so to protect themselves people say "I can't."Another related line you often hear is that not everyone can do
work they love—that someone has to do the unpleasant jobs.  Really?
How do you make them?  In the US the only mechanism for forcing
people to do unpleasant jobs is the draft, and that hasn't been
invoked for over 30 years.  All we can do is encourage people to
do unpleasant work, with money and prestige.If there's something people still won't do, it seems as if society
just has to make do without.  That's what happened with domestic
servants.  For millennia that was the canonical example of a job
"someone had to do."  And yet in the mid twentieth century servants
practically disappeared in rich countries, and the rich have just
had to do without.So while there may be some things someone has to do, there's a good
chance anyone saying that about any particular job is mistaken.
Most unpleasant jobs would either get automated or go undone if no
one were willing to do them.Two RoutesThere's another sense of "not everyone can do work they love"
that's all too true, however.  One has to make a living, and it's
hard to get paid for doing work you love.  There are two routes to
that destination:

  The organic route: as you become more eminent, gradually to
  increase the parts of your job that you like at the expense of
  those you don't.The two-job route: to work at things you don't like to get money
  to work on things you do.

The organic route is more common.  It happens naturally to anyone
who does good work.  A young architect has to take whatever work
he can get, but if he does well he'll gradually be in a position
to pick and choose among projects.  The disadvantage of this route
is that it's slow and uncertain.  Even tenure is not real freedom.The two-job route has several variants depending on how long you
work for money at a time.  At one extreme is the "day job," where
you work regular hours at one job to make money, and work on what
you love in your spare time.  At the other extreme you work at
something till you make enough not to 
have to work for money again.The two-job route is less common than the organic route, because
it requires a deliberate choice.  It's also more dangerous.  Life
tends to get more expensive as you get older, so it's easy to get
sucked into working longer than you expected at the money job.
Worse still, anything you work on changes you.  If you work too
long on tedious stuff, it will rot your brain.  And the best paying
jobs are most dangerous, because they require your full attention.The advantage of the two-job route is that it lets you jump over
obstacles.  The landscape of possible jobs isn't flat; there are
walls of varying heights between different kinds of work. 
[7]
The trick of maximizing the parts of your job that you like can get you
from architecture to product design, but not, probably, to music.
If you make money doing one thing and then work on another, you
have more freedom of choice.Which route should you take?  That depends on how sure you are of
what you want to do, how good you are at taking orders, how much
risk you can stand, and the odds that anyone will pay (in your
lifetime) for what you want to do.  If you're sure of the general
area you want to work in and it's something people are likely to
pay you for, then you should probably take the organic route.  But
if you don't know what you want to work on, or don't like to take
orders, you may want to take the two-job route, if you can stand
the risk.Don't decide too soon.  Kids who know early what they want to do
seem impressive, as if they got the answer to some math question
before the other kids.  They have an answer, certainly, but odds
are it's wrong.A friend of mine who is a quite successful doctor complains constantly
about her job.  When people applying to medical school ask her for
advice, she wants to shake them and yell "Don't do it!"  (But she
never does.) How did she get into this fix?  In high school she
already wanted to be a doctor.  And she is so ambitious and determined
that she overcame every obstacle along the way—including,
unfortunately, not liking it.Now she has a life chosen for her by a high-school kid.When you're young, you're given the impression that you'll get
enough information to make each choice before you need to make it.
But this is certainly not so with work.  When you're deciding what
to do, you have to operate on ridiculously incomplete information.
Even in college you get little idea what various types of work are
like.  At best you may have a couple internships, but not all jobs
offer internships, and those that do don't teach you much more about
the work than being a batboy teaches you about playing baseball.In the design of lives, as in the design of most other things, you
get better results if you use flexible media.  So unless you're
fairly sure what you want to do, your best bet may be to choose a
type of work that could turn into either an organic or two-job
career.  That was probably part of the reason I chose computers.
You can be a professor, or make a lot of money, or morph it into
any number of other kinds of work.It's also wise, early on, to seek jobs that let you do many different
things, so you can learn faster what various kinds of work are like.
Conversely, the extreme version of the two-job route is dangerous
because it teaches you so little about what you like.  If you work
hard at being a bond trader for ten years, thinking that you'll
quit and write novels when you have enough money, what happens when
you quit and then discover that you don't actually like writing
novels?Most people would say, I'd take that problem.  Give me a million
dollars and I'll figure out what to do.  But it's harder than it
looks.  Constraints give your life shape.  Remove them and most
people have no idea what to do: look at what happens to those who
win lotteries or inherit money.  Much as everyone thinks they want
financial security, the happiest people are not those who have it,
but those who like what they do.  So a plan that promises freedom
at the expense of knowing what to do with it may not be as good as
it seems.Whichever route you take, expect a struggle.  Finding work you love
is very difficult.  Most people fail.  Even if you succeed, it's
rare to be free to work on what you want till your thirties or
forties.  But if you have the destination in sight you'll be more
likely to arrive at it.  If you know you can love work, you're in
the home stretch, and if you know what work you love, you're
practically there.Notes[1]
Currently we do the opposite: when we make kids do boring work,
like arithmetic drills, instead of admitting frankly that it's
boring, we try to disguise it with superficial decorations.[2]
One father told me about a related phenomenon: he found himself
concealing from his family how much he liked his work.  When he
wanted to go to work on a saturday, he found it easier to say that
it was because he "had to" for some reason, rather than admitting
he preferred to work than stay home with them.[3]
Something similar happens with suburbs.  Parents move to suburbs
to raise their kids in a safe environment, but suburbs are so dull
and artificial that by the time they're fifteen the kids are convinced
the whole world is boring.[4]
I'm not saying friends should be the only audience for your
work.  The more people you can help, the better.  But friends should
be your compass.[5]
Donald Hall said young would-be poets were mistaken to be so
obsessed with being published.  But you can imagine what it would
do for a 24 year old to get a poem published in The New Yorker.
Now to people he meets at parties he's a real poet.  Actually he's
no better or worse than he was before, but to a clueless audience
like that, the approval of an official authority makes all the
difference.   So it's a harder problem than Hall realizes.  The
reason the young care so much about prestige is that the people
they want to impress are not very discerning.[6]
This is isomorphic to the principle that you should prevent
your beliefs about how things are from being contaminated by how
you wish they were.  Most people let them mix pretty promiscuously.
The continuing popularity of religion is the most visible index of
that.[7]
A more accurate metaphor would be to say that the graph of jobs
is not very well connected.Thanks to Trevor Blackwell, Dan Friedman, Sarah Harlin,
Jessica Livingston, Jackie McDonough, Robert Morris, Peter Norvig, 
David Sloo, and Aaron Swartz
for reading drafts of this.January 2003(This article is derived from a keynote talk at the fall 2002 meeting
of NEPLS.)Visitors to this country are often surprised to find that
Americans like to begin a conversation by asking "what do you do?"
I've never liked this question.  I've rarely had a
neat answer to it.  But I think I have finally solved the problem.
Now, when someone asks me what I do, I look them straight
in the eye and say "I'm designing a 
new dialect of Lisp."   
I recommend this answer to anyone who doesn't like being asked what
they do.  The conversation will turn immediately to other topics.I don't consider myself to be doing research on programming languages.
I'm just designing one, in the same way that someone might design
a building or a chair or a new typeface.
I'm not trying to discover anything new.  I just want
to make a language that will be good to program in.  In some ways,
this assumption makes life a lot easier.The difference between design and research seems to be a question
of new versus good.  Design doesn't have to be new, but it has to  
be good.  Research doesn't have to be good, but it has to be new.
I think these two paths converge at the top: the best design
surpasses its predecessors by using new ideas, and the best research
solves problems that are not only new, but actually worth solving.
So ultimately we're aiming for the same destination, just approaching
it from different directions.What I'm going to talk about today is what your target looks like
from the back.  What do you do differently when you treat
programming languages as a design problem instead of a research topic?The biggest difference is that you focus more on the user.
Design begins by asking, who is this
for and what do they need from it?  A good architect,
for example, does not begin by creating a design that he then
imposes on the users, but by studying the intended users and figuring
out what they need.Notice I said "what they need," not "what they want."  I don't mean
to give the impression that working as a designer means working as 
a sort of short-order cook, making whatever the client tells you
to.  This varies from field to field in the arts, but
I don't think there is any field in which the best work is done by
the people who just make exactly what the customers tell them to.The customer is always right in
the sense that the measure of good design is how well it works
for the user.  If you make a novel that bores everyone, or a chair
that's horribly uncomfortable to sit in, then you've done a bad
job, period.  It's no defense to say that the novel or the chair  
is designed according to the most advanced theoretical principles.And yet, making what works for the user doesn't mean simply making
what the user tells you to.  Users don't know what all the choices
are, and are often mistaken about what they really want.The answer to the paradox, I think, is that you have to design
for the user, but you have to design what the user needs, not simply  
what he says he wants.
It's much like being a doctor.  You can't just treat a patient's
symptoms.  When a patient tells you his symptoms, you have to figure
out what's actually wrong with him, and treat that.This focus on the user is a kind of axiom from which most of the
practice of good design can be derived, and around which most design
issues center.If good design must do what the user needs, who is the user?  When
I say that design must be for users, I don't mean to imply that good 
design aims at some kind of  
lowest common denominator.  You can pick any group of users you
want.  If you're designing a tool, for example, you can design it
for anyone from beginners to experts, and what's good design
for one group might be bad for another.  The point
is, you have to pick some group of users.  I don't think you can
even talk about good or bad design except with
reference to some intended user.You're most likely to get good design if the intended users include
the designer himself.  When you design something
for a group that doesn't include you, it tends to be for people
you consider to be less sophisticated than you, not more sophisticated.That's a problem, because looking down on the user, however benevolently,
seems inevitably to corrupt the designer.
I suspect that very few housing
projects in the US were designed by architects who expected to live
in them.   You can see the same thing
in programming languages.  C, Lisp, and Smalltalk were created for
their own designers to use.  Cobol, Ada, and Java, were created   
for other people to use.If you think you're designing something for idiots, the odds are
that you're not designing something good, even for idiots.
Even if you're designing something for the most sophisticated
users, though, you're still designing for humans.  It's different 
in research.  In math you
don't choose abstractions because they're
easy for humans to understand; you choose whichever make the
proof shorter.  I think this is true for the sciences generally.
Scientific ideas are not meant to be ergonomic.Over in the arts, things are very different.  Design is
all about people.  The human body is a strange
thing, but when you're designing a chair,
that's what you're designing for, and there's no way around it.
All the arts have to pander to the interests and limitations
of humans.   In painting, for example, all other things being
equal a painting with people in it will be more interesting than
one without.  It is not merely an accident of history that
the great paintings of the Renaissance are all full of people.
If they hadn't been, painting as a medium wouldn't have the prestige
that it does.Like it or not, programming languages are also for people,
and I suspect the human brain is just as lumpy and idiosyncratic
as the human body.  Some ideas are easy for people to grasp
and some aren't.  For example, we seem to have a very limited
capacity for dealing with detail.  It's this fact that makes
programing languages a good idea in the first place; if we
could handle the detail, we could just program in machine
language.Remember, too, that languages are not
primarily a form for finished programs, but something that
programs have to be developed in.  Anyone in the arts could
tell you that you might want different mediums for the
two situations.  Marble, for example, is a nice, durable
medium for finished ideas, but a hopelessly inflexible one
for developing new ideas.A program, like a proof,
is a pruned version of a tree that in the past has had
false starts branching off all over it.  So the test of
a language is not simply how clean the finished program looks
in it, but how clean the path to the finished program was.
A design choice that gives you elegant finished programs
may not give you an elegant design process.  For example, 
I've written a few macro-defining macros full of nested
backquotes that look now like little gems, but writing them
took hours of the ugliest trial and error, and frankly, I'm still
not entirely sure they're correct.We often act as if the test of a language were how good
finished programs look in it.
It seems so convincing when you see the same program
written in two languages, and one version is much shorter.
When you approach the problem from the direction of the
arts, you're less likely to depend on this sort of
test.  You don't want to end up with a programming
language like marble.For example, it is a huge win in developing software to
have an interactive toplevel, what in Lisp is called a
read-eval-print loop.  And when you have one this has
real effects on the design of the language.  It would not
work well for a language where you have to declare
variables before using them, for example.  When you're
just typing expressions into the toplevel, you want to be 
able to set x to some value and then start doing things
to x.  You don't want to have to declare the type of x
first.  You may dispute either of the premises, but if
a language has to have a toplevel to be convenient, and
mandatory type declarations are incompatible with a
toplevel, then no language that makes type declarations  
mandatory could be convenient to program in.In practice, to get good design you have to get close, and stay
close, to your users.  You have to calibrate your ideas on actual
users constantly, especially in the beginning.  One of the reasons
Jane Austen's novels are so good is that she read them out loud to
her family.  That's why she never sinks into self-indulgently arty
descriptions of landscapes,
or pretentious philosophizing.  (The philosophy's there, but it's
woven into the story instead of being pasted onto it like a label.)
If you open an average "literary" novel and imagine reading it out loud
to your friends as something you'd written, you'll feel all too
keenly what an imposition that kind of thing is upon the reader.In the software world, this idea is known as Worse is Better.
Actually, there are several ideas mixed together in the concept of
Worse is Better, which is why people are still arguing about
whether worse
is actually better or not.  But one of the main ideas in that
mix is that if you're building something new, you should get a
prototype in front of users as soon as possible.The alternative approach might be called the Hail Mary strategy.
Instead of getting a prototype out quickly and gradually refining
it, you try to create the complete, finished, product in one long
touchdown pass.  As far as I know, this is a
recipe for disaster.  Countless startups destroyed themselves this
way during the Internet bubble.  I've never heard of a case
where it worked.What people outside the software world may not realize is that
Worse is Better is found throughout the arts.
In drawing, for example, the idea was discovered during the
Renaissance.  Now almost every drawing teacher will tell you that
the right way to get an accurate drawing is not to
work your way slowly around the contour of an object, because errors will
accumulate and you'll find at the end that the lines don't meet.
Instead you should draw a few quick lines in roughly the right place,
and then gradually refine this initial sketch.In most fields, prototypes
have traditionally been made out of different materials.
Typefaces to be cut in metal were initially designed  
with a brush on paper.  Statues to be cast in bronze   
were modelled in wax.  Patterns to be embroidered on tapestries
were drawn on paper with ink wash.  Buildings to be
constructed from stone were tested on a smaller scale in wood.What made oil paint so exciting, when it
first became popular in the fifteenth century, was that you
could actually make the finished work from the prototype.
You could make a preliminary drawing if you wanted to, but you
weren't held to it; you could work out all the details, and
even make major changes, as you finished the painting.You can do this in software too.  A prototype doesn't have to
be just a model; you can refine it into the finished product.
I think you should always do this when you can.  It lets you
take advantage of new insights you have along the way.  But
perhaps even more important, it's good for morale.Morale is key in design.  I'm surprised people
don't talk more about it.  One of my first
drawing teachers told me: if you're bored when you're
drawing something, the drawing will look boring.
For example, suppose you have to draw a building, and you
decide to draw each brick individually.  You can do this
if you want, but if you get bored halfway through and start
making the bricks mechanically instead of observing each one,   
the drawing will look worse than if you had merely suggested
the bricks.Building something by gradually refining a prototype is good
for morale because it keeps you engaged.  In software, my  
rule is: always have working code.  If you're writing
something that you'll be able to test in an hour, then you
have the prospect of an immediate reward to motivate you.
The same is true in the arts, and particularly in oil painting.
Most painters start with a blurry sketch and gradually
refine it.
If you work this way, then in principle
you never have to end the day with something that actually
looks unfinished.  Indeed, there is even a saying among
painters: "A painting is never finished, you just stop
working on it."  This idea will be familiar to anyone who
has worked on software.Morale is another reason that it's hard to design something
for an unsophisticated user.   It's hard to stay interested in
something you don't like yourself.  To make something  
good, you have to be thinking, "wow, this is really great,"
not "what a piece of shit; those fools will love it."Design means making things for humans.  But it's not just the
user who's human.  The designer is human too.Notice all this time I've been talking about "the designer."
Design usually has to be under the control of a single person to
be any good.   And yet it seems to be possible for several people
to collaborate on a research project.  This seems to
me one of the most interesting differences between research and
design.There have been famous instances of collaboration in the arts,
but most of them seem to have been cases of molecular bonding rather
than nuclear fusion.  In an opera it's common for one person to
write the libretto and another to write the music.   And during the Renaissance, 
journeymen from northern
Europe were often employed to do the landscapes in the
backgrounds of Italian paintings.  But these aren't true collaborations.
They're more like examples of Robert Frost's
"good fences make good neighbors."  You can stick instances
of good design together, but within each individual project,
one person has to be in control.I'm not saying that good design requires that one person think
of everything.  There's nothing more valuable than the advice
of someone whose judgement you trust.  But after the talking is
done, the decision about what to do has to rest with one person.Why is it that research can be done by collaborators and  
design can't?  This is an interesting question.  I don't 
know the answer.  Perhaps,
if design and research converge, the best research is also
good design, and in fact can't be done by collaborators.
A lot of the most famous scientists seem to have worked alone.
But I don't know enough to say whether there
is a pattern here.  It could be simply that many famous scientists
worked when collaboration was less common.Whatever the story is in the sciences, true collaboration
seems to be vanishingly rare in the arts.  Design by committee is a
synonym for bad design.  Why is that so?  Is there some way to
beat this limitation?I'm inclined to think there isn't-- that good design requires
a dictator.  One reason is that good design has to   
be all of a piece.  Design is not just for humans, but
for individual humans.  If a design represents an idea that  
fits in one person's head, then the idea will fit in the user's
head too.Related:September 2007In high school I decided I was going to study philosophy in college.
I had several motives, some more honorable than others.  One of the
less honorable was to shock people.  College was regarded as job
training where I grew up, so studying philosophy seemed an impressively
impractical thing to do.  Sort of like slashing holes in your clothes
or putting a safety pin through your ear, which were other forms
of impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying
philosophy would be a shortcut straight to wisdom.  All the people
majoring in other things would just end up with a bunch of domain
knowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you
wouldn't find those in our high school library.  But I tried to
read Plato and Aristotle.  I doubt I believed I understood them,
but they sounded like they were talking about something important.
I assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned
a lot in the calculus class, but I didn't learn much in Philosophy
101.  And yet my plan to study philosophy remained intact.  It was
my fault I hadn't learned anything.  I hadn't read the books we
were assigned carefully enough.  I'd give Berkeley's Principles
of Human Knowledge another shot in college.  Anything so admired
and so difficult to read must have something in it, if one could
only figure out what.Twenty-six years later, I still don't understand Berkeley.  I have
a nice edition of his collected works.  Will I ever read it?  Seems
unlikely.The difference between then and now is that now I understand why
Berkeley is probably not worth trying to understand.  I think I see
now what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It
didn't work out as I'd hoped.  I didn't learn any magical truths
compared to which everything else was mere domain knowledge.  But
I do at least know now why I didn't.  Philosophy doesn't really
have a subject matter in the way math or history or most other
university subjects do.  There is no core of knowledge one must
master.  The closest you come to that is a knowledge of what various
individual philosophers have said about different topics over the
years.  Few were sufficiently correct that people have forgotten
who discovered what they discovered.Formal logic has some subject matter. I took several classes in
logic.  I don't know if I learned anything from them.
[1]
It does seem to me very important to be able to flip ideas around in
one's head: to see when two ideas don't fully cover the space of
possibilities, or when one idea is the same as another but with a
couple things changed.  But did studying logic teach me the importance
of thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The
most dramatic I learned immediately, in the first semester of
freshman year, in a class taught by Sydney Shoemaker.  I learned
that I don't exist.  I am (and you are) a collection of cells that
lurches around driven by various forces, and calls itself I.  But
there's no central, indivisible thing that your identity goes with.
You could conceivably lose half your brain and live.  Which means
your brain could conceivably be split into two halves and each
transplanted into different bodies.  Imagine waking up after such
an operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life
are fuzzy, and break down if pushed too hard.  Even a concept as
dear to us as I.  It took me a while to grasp this, but when I
did it was fairly sudden, like someone in the nineteenth century
grasping evolution and realizing the story of creation they'd been
told as a child was all wrong. 
[2]
Outside of math there's a limit
to how far you can push words; in fact, it would not be a bad
definition of math to call it the study of terms that have precise
meanings.  Everyday words are inherently imprecise.  They work well
enough in everyday life that you don't notice.  Words seem to work,
just as Newtonian physics seems to.  But you can always make them
break if you push them far enough.I would say that this has been, unfortunately for philosophy, the
central fact of philosophy.  Most philosophical debates are not
merely afflicted by but driven by confusions over words.  Do we
have free will?  Depends what you mean by "free." Do abstract ideas
exist?  Depends what you mean by "exist."Wittgenstein is popularly credited with the idea that most philosophical
controversies are due to confusions over language.  I'm not sure
how much credit to give him.  I suspect a lot of people realized
this, but reacted simply by not studying philosophy, rather than
becoming philosophy professors.How did things get this way?  Can something people have spent
thousands of years studying really be a waste of time?  Those are
interesting questions.  In fact, some of the most interesting
questions you can ask about philosophy.  The most valuable way to
approach the current philosophical tradition may be neither to get
lost in pointless speculations like Berkeley, nor to shut them down
like Wittgenstein, but to study it as an example of reason gone
wrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle.
What we know of their predecessors comes from fragments and references
in later works; their doctrines could be described as speculative
cosmology that occasionally strays into analysis.  Presumably they
were driven by whatever makes people in every other society invent
cosmologies.
[3]With Socrates, Plato, and particularly Aristotle, this tradition
turned a corner.  There started to be a lot more analysis.  I suspect
Plato and Aristotle were encouraged in this by progress in math.
Mathematicians had by then shown that you could figure things out
in a much more conclusive way than by making up fine sounding stories
about them.  
[4]People talk so much about abstractions now that we don't realize
what a leap it must have been when they first started to.  It was
presumably many thousands of years between when people first started
describing things as hot or cold and when someone asked "what is
heat?"  No doubt it was a very gradual process.  We don't know if
Plato or Aristotle were the first to ask any of the questions they
did.  But their works are the oldest we have that do this on a large
scale, and there is a freshness (not to say naivete) about them
that suggests some of the questions they asked were new to them,
at least.Aristotle in particular reminds me of the phenomenon that happens
when people discover something new, and are so excited by it that
they race through a huge percentage of the newly discovered territory
in one lifetime.  If so, that's evidence of how new this kind of
thinking was. 
[5]This is all to explain how Plato and Aristotle can be very impressive
and yet naive and mistaken.  It was impressive even to ask the
questions they did.  That doesn't mean they always came up with
good answers.  It's not considered insulting to say that ancient
Greek mathematicians were naive in some respects, or at least lacked
some concepts that would have made their lives easier.  So I hope
people will not be too offended if I propose that ancient philosophers
were similarly naive.  In particular, they don't seem to have fully
grasped what I earlier called the central fact of philosophy: that
words break if you push them too far."Much to the surprise of the builders of the first digital computers,"
Rod Brooks wrote, "programs written for them usually did not work."
[6]
Something similar happened when people first started trying
to talk about abstractions.  Much to their surprise, they didn't
arrive at answers they agreed upon.  In fact, they rarely seemed
to arrive at answers at all.They were in effect arguing about artifacts induced by sampling at
too low a resolution.The proof of how useless some of their answers turned out to be is
how little effect they have.  No one after reading Aristotle's
Metaphysics does anything differently as a result.
[7]Surely I'm not claiming that ideas have to have practical applications
to be interesting?  No, they may not have to.  Hardy's boast that
number theory had no use whatsoever wouldn't disqualify it.  But
he turned out to be mistaken.  In fact, it's suspiciously hard to
find a field of math that truly has no practical use.  And Aristotle's
explanation of the ultimate goal of philosophy in Book A of the
Metaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.
The examples he gives are convincing: an ordinary worker builds
things a certain way out of habit; a master craftsman can do more
because he grasps the underlying principles.  The trend is clear:
the more general the knowledge, the more admirable it is.  But then
he makes a mistake—possibly the most important mistake in the
history of philosophy.  He has noticed that theoretical knowledge
is often acquired for its own sake, out of curiosity, rather than
for any practical need.  So he proposes there are two kinds of
theoretical knowledge: some that's useful in practical matters and
some that isn't.  Since people interested in the latter are interested
in it for its own sake, it must be more noble.  So he sets as his
goal in the Metaphysics the exploration of knowledge that has no
practical use.  Which means no alarms go off when he takes on grand
but vaguely understood questions and ends up getting lost in a sea
of words.His mistake was to confuse motive and result.  Certainly, people
who want a deep understanding of something are often driven by
curiosity rather than any practical need.  But that doesn't mean
what they end up learning is useless.  It's very valuable in practice
to have a deep understanding of what you're doing; even if you're
never called on to solve advanced problems, you can see shortcuts
in the solution of simple ones, and your knowledge won't break down
in edge cases, as it would if you were relying on formulas you
didn't understand.  Knowledge is power.  That's what makes theoretical
knowledge prestigious.  It's also what causes smart people to be
curious about certain things and not others; our DNA is not so
disinterested as we might think.So while ideas don't have to have immediate practical applications
to be interesting, the kinds of things we find interesting will
surprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was
partly that he set off with contradictory aims: to explore the most
abstract ideas, guided by the assumption that they were useless.
He was like an explorer looking for a territory to the north of
him, starting with the assumption that it was located to the south.And since his work became the map used by generations of future
explorers, he sent them off in the wrong direction as well. 
[8]
Perhaps worst of all, he protected them from both the criticism of
outsiders and the promptings of their own inner compass by establishing
the principle that the most noble sort of theoretical knowledge had
to be useless.The Metaphysics is mostly a failed experiment.  A few ideas from
it turned out to be worth keeping; the bulk of it has had no effect
at all.  The Metaphysics is among the least read of all famous
books.  It's not hard to understand the way Newton's Principia
is, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately
that was not the conclusion Aristotle's successors derived from
works like the Metaphysics. 
[9]
Soon after, the western world
fell on intellectual hard times.  Instead of version 1s to be
superseded, the works of Plato and Aristotle became revered texts
to be mastered and discussed.  And so things remained for a shockingly
long time.  It was not till around 1600 (in Europe, where the center
of gravity had shifted by then) that one found people confident
enough to treat Aristotle's work as a catalog of mistakes.  And
even then they rarely said so outright.If it seems surprising that the gap was so long, consider how little
progress there was in math between Hellenistic times and the
Renaissance.In the intervening years an unfortunate idea took hold:  that it
was not only acceptable to produce works like the Metaphysics,
but that it was a particularly prestigious line of work, done by a
class of people called philosophers.  No one thought to go back and
debug Aristotle's motivating argument.  And so instead of correcting
the problem Aristotle discovered by falling into it—that you can
easily get lost if you talk too loosely about very abstract ideas—they 
continued to fall into it.The SingularityCuriously, however, the works they produced continued to attract
new readers.  Traditional philosophy occupies a kind of singularity
in this respect.  If you write in an unclear way about big ideas,
you produce something that seems tantalizingly attractive to
inexperienced but intellectually ambitious students.  Till one knows
better, it's hard to distinguish something that's hard to understand
because the writer was unclear in his own mind from something like
a mathematical proof that's hard to understand because the ideas
it represents are hard to understand.  To someone who hasn't learned
the difference, traditional philosophy seems extremely attractive:
as hard (and therefore impressive) as math, yet broader in scope.
That was what lured me in as a high school student.This singularity is even more singular in having its own defense
built in.  When things are hard to understand, people who suspect
they're nonsense generally keep quiet.  There's no way to prove a
text is meaningless.  The closest you can get is to show that the
official judges of some class of texts can't distinguish them from
placebos. 
[10]And so instead of denouncing philosophy, most people who suspected
it was a waste of time just studied other things.  That alone is
fairly damning evidence, considering philosophy's claims.  It's
supposed to be about the ultimate truths. Surely all smart people
would be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might
have corrected them, they tended to be self-perpetuating.  Bertrand
Russell wrote in a letter in 1912:

  Hitherto the people attracted to philosophy have been mostly those
  who loved the big generalizations, which were all wrong, so that
  few people with exact minds have taken up the subject.
[11]

His response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery
that most previous philosophy was a waste of time, which judging
from the circumstantial evidence must have been made by every smart
person who studied a little philosophy and declined to pursue it
further, but for how he acted in response.
[12]
Instead of quietly
switching to another field, he made a fuss, from inside.  He was
Gorbachev.The field of philosophy is still shaken from the fright Wittgenstein
gave it. 
[13]
Later in life he spent a lot of time talking about
how words worked.  Since that seems to be allowed, that's what a
lot of philosophers do now.  Meanwhile, sensing a vacuum in the
metaphysical speculation department, the people who used to do
literary criticism have been edging Kantward, under new names like
"literary theory," "critical theory," and when they're feeling
ambitious, plain "theory."  The writing is the familiar word salad:

  Gender is not like some of the other grammatical modes which
  express precisely a mode of conception without any reality that
  corresponds to the conceptual mode, and consequently do not express
  precisely something in reality by which the intellect could be
  moved to conceive a thing the way it does, even where that motive
  is not something in the thing as such.
  [14]

The singularity I've described is not going away.  There's a market
for writing that sounds impressive and can't be disproven. There
will always be both supply and demand.  So if one group abandons
this territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility.
Perhaps we should do what Aristotle meant to do, instead of what
he did.  The goal he announces in the Metaphysics seems one worth
pursuing: to discover the most general truths.  That sounds good.
But instead of trying to discover them because they're useless,
let's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised
criterion, applicability, as a guide to keep us from wondering
off into a swamp of abstractions.  Instead of trying to answer the
question:

  What are the most general truths?

let's try to answer the question

  Of all the useful things we can say, which are the most general?

The test of utility I propose is whether we cause people who read
what we've written to do anything differently afterward.  Knowing
we have to give definite (if implicit) advice will keep us from
straying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a
different direction.As an example of a useful, general idea, consider that of the
controlled experiment.  There's an idea that has turned out to be
widely applicable.  Some might say it's part of science, but it's
not part of any specific science; it's literally meta-physics (in
our sense of "meta").   The idea of evolution is another. It turns
out to have quite broad applications—for example, in genetic
algorithms and even product design.  Frankfurt's distinction between
lying and bullshitting seems a promising recent example.
[15]These seem to me what philosophy should look like: quite general
observations that would cause someone who understood them to do
something differently.Such observations will necessarily be about things that are imprecisely
defined.  Once you start using words with precise meanings, you're
doing math.  So starting from utility won't entirely solve the
problem I described above—it won't flush out the metaphysical
singularity.  But it should help.  It gives people with good
intentions a new roadmap into abstraction.  And they may thereby
produce things that make the writing of the people with bad intentions
look bad by comparison.One drawback of this approach is that it won't produce the sort of
writing that gets you tenure.  And not just because it's not currently
the fashion.  In order to get tenure in any field you must not
arrive at conclusions that members of tenure committees can disagree
with.  In practice there are two kinds of solutions to this problem.
In math and the sciences, you can prove what you're saying, or at
any rate adjust your conclusions so you're not claiming anything
false ("6 of 8 subjects had lower blood pressure after the treatment").
In the humanities you can either avoid drawing any definite conclusions
(e.g. conclude that an issue is a complex one), or draw conclusions
so narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either
of these routes.  At best you'll be able to achieve the essayist's
standard of proof, not the mathematician's or the experimentalist's.
And yet you won't be able to meet the usefulness test without
implying definite and fairly broadly applicable conclusions.  Worse
still, the usefulness test will tend to produce results that annoy
people: there's no use in telling people things they already believe,
and people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting
to general plus useful by starting with useful and cranking up the
generality may be unsuitable for junior professors trying to get
tenure, but it's better for everyone else, including professors who
already have it.  This side of the mountain is a nice gradual slope.
You can start by writing things that are useful but very specific,
and then gradually make them more general.  Joe's has good burritos.
What makes a good burrito?  What makes good food?  What makes
anything good?  You can take as long as you want.  You don't have
to get all the way to the top of the mountain.  You don't have to
tell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an
encouraging thought.  The field is a lot younger than it seems.
Though the first philosophers in the western tradition lived about
2500 years ago, it would be misleading to say the field is 2500
years old, because for most of that time the leading practitioners
weren't doing much more than writing commentaries on Plato or
Aristotle while watching over their shoulders for the next invading
army.  In the times when they weren't, philosophy was hopelessly
intermingled with religion.  It didn't shake itself free till a
couple hundred years ago, and even then was afflicted by the
structural problems I've described above.  If I say this, some will
say it's a ridiculously overbroad and uncharitable generalization,
and others will say it's old news, but here goes: judging from their
works, most philosophers up to the present have been wasting their
time.  So in a sense the field is still at the first step. 
[16]That sounds a preposterous claim to make.  It won't seem so
preposterous in 10,000 years.  Civilization always seems old, because
it's always the oldest it's ever been.  The only way to say whether
something is really old or not is by looking at structural evidence,
and structurally philosophy is young; it's still reeling from the
unexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot
more to discover.Notes
[1]
In practice formal logic is not much use, because despite
some progress in the last 150 years we're still only able to formalize
a small percentage of statements.  We may never do that much better,
for the same reason 1980s-style "knowledge representation" could
never have worked; many statements may have no representation more
concise than a huge, analog brain state.[2]
It was harder for Darwin's contemporaries to grasp this than
we can easily imagine.  The story of creation in the Bible is not
just a Judeo-Christian concept; it's roughly what everyone must
have believed since before people were people.  The hard part of
grasping evolution was to realize that species weren't, as they
seem to be, unchanging, but had instead evolved from different,
simpler organisms over unimaginably long periods of time.Now we don't have to make that leap.  No one in an industrialized
country encounters the idea of evolution for the first time as an
adult.  Everyone's taught about it as a child, either as truth or
heresy.[3]
Greek philosophers before Plato wrote in verse.  This must
have affected what they said.  If you try to write about the nature
of the world in verse, it inevitably turns into incantation.  Prose
lets you be more precise, and more tentative.[4]
Philosophy is like math's
ne'er-do-well brother.  It was born when Plato and Aristotle looked
at the works of their predecessors and said in effect "why can't
you be more like your brother?"  Russell was still saying the same
thing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy
the imprecise half.  It's probably inevitable that philosophy will
suffer by comparison, because there's no lower bound to its precision.
Bad math is merely boring, whereas bad philosophy is nonsense.  And
yet there are some good ideas in the imprecise half.[5]
Aristotle's best work was in logic and zoology, both of which
he can  be said to have invented.  But the most dramatic departure
from his predecessors was a new, much more analytical style of
thinking.  He was arguably the first scientist.[6]
Brooks, Rodney, Programming in Common Lisp, Wiley, 1985, p.
94.[7]
Some would say we depend on Aristotle more than we realize,
because his ideas were one of the ingredients in our common culture.
Certainly a lot of the words we use have a connection with Aristotle,
but it seems a bit much to suggest that we wouldn't have the concept
of the essence of something or the distinction between matter and
form if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to
diff European culture with Chinese: what ideas did European culture
have in 1800 that Chinese culture didn't, in virtue of Aristotle's
contribution?[8]
The meaning of the word "philosophy" has changed over time.
In ancient times it covered a broad range of topics, comparable in
scope to our "scholarship" (though without the methodological
implications).  Even as late as Newton's time it included what we
now call "science."  But core of the subject today is still what
seemed to Aristotle the core: the attempt to discover the most
general truths.Aristotle didn't call this "metaphysics."  That name got assigned
to it because the books we now call the Metaphysics came after
(meta = after) the Physics in the standard edition of Aristotle's
works compiled by Andronicus of Rhodes three centuries later.  What
we call "metaphysics" Aristotle called "first philosophy."[9]
Some of Aristotle's immediate successors may have realized
this, but it's hard to say because most of their works are lost.[10]
Sokal, Alan, "Transgressing the Boundaries: Toward a Transformative
Hermeneutics of Quantum Gravity," Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's
aligned with some axe the audience already has to grind.  If this
is so we should find it's most popular with groups that are (or
feel) weak.  The powerful don't need its reassurance.[11]
Letter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991,
p. 75.[12]
A preliminary result, that all metaphysics between Aristotle
and 1783 had been a waste of time, is due to I. Kant.[13]
Wittgenstein asserted a sort of mastery to which the inhabitants
of early 20th century Cambridge seem to have been peculiarly
vulnerable—perhaps partly because so many had been raised religious
and then stopped believing, so had a vacant space in their heads
for someone to tell them what to do (others chose Marx or Cardinal
Newman), and partly because a quiet, earnest place like Cambridge
in that era had no natural immunity to messianic figures, just as
European politics then had no natural immunity to dictators.[14]
This is actually from the Ordinatio of Duns Scotus (ca.
1300), with "number" replaced by "gender."  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson,
1963, p. 92.[15]
Frankfurt, Harry, On Bullshit,  Princeton University Press,
2005.[16]
Some introductions to philosophy now take the line that
philosophy is worth studying as a process rather than for any
particular truths you'll learn.  The philosophers whose works they
cover would be rolling in their graves at that.  They hoped they
were doing more than serving as examples of how to argue: they hoped
they were getting results.  Most were wrong, but it doesn't seem
an impossible hope.This argument seems to me like someone in 1500 looking at the lack
of results achieved by alchemy and saying its value was as a process.
No, they were going about it wrong.  It turns out it is possible
to transmute lead into gold (though not economically at current
energy prices), but the route to that knowledge was to
backtrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston, 
Robert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.May 2001(This article was written as a kind of business plan for a
new language.
So it is missing (because it takes for granted) the most important
feature of a good programming language: very powerful abstractions.)A friend of mine once told an eminent operating systems
expert that he wanted to design a really good
programming language.  The expert told him that it would be a
waste of time, that programming languages don't become popular
or unpopular based on their merits, and so no matter how
good his language was, no one would use it.  At least, that
was what had happened to the language he had designed.What does make a language popular?  Do popular
languages deserve their popularity?  Is it worth trying to
define a good programming language?  How would you do it?I think the answers to these questions can be found by looking 
at hackers, and learning what they want.  Programming
languages are for hackers, and a programming language
is good as a programming language (rather than, say, an
exercise in denotational semantics or compiler design)
if and only if hackers like it.1 The Mechanics of PopularityIt's true, certainly, that most people don't choose programming
languages simply based on their merits.  Most programmers are told
what language to use by someone else.  And yet I think the effect
of such external factors on the popularity of programming languages
is not as great as it's sometimes thought to be. I think a bigger
problem is that a hacker's idea of a good programming language is
not the same as most language designers'.Between the two, the hacker's opinion is the one that matters.
Programming languages are not theorems. They're tools, designed
for people, and they have to be designed to suit human strengths
and weaknesses as much as shoes have to be designed for human feet.
If a shoe pinches when you put it on, it's a bad shoe, however
elegant it may be as a piece of sculpture.It may be that the majority of programmers can't tell a good language
from a bad one. But that's no different with any other tool. It
doesn't mean that it's a waste of time to try designing a good
language. Expert hackers 
can tell a good language when they see
one, and they'll use it. Expert hackers are a tiny minority,
admittedly, but that tiny minority write all the good software,
and their influence is such that the rest of the programmers will
tend to use whatever language they use. Often, indeed, it is not
merely influence but command: often the expert hackers are the very
people who, as their bosses or faculty advisors, tell the other
programmers what language to use.The opinion of expert hackers is not the only force that determines
the relative popularity of programming languages — legacy software
(Cobol) and hype (Ada, Java) also play a role — but I think it is
the most powerful force over the long term. Given an initial critical
mass and enough time, a programming language probably becomes about
as popular as it deserves to be. And popularity further separates
good languages from bad ones, because feedback from real live users
always leads to improvements. Look at how much any popular language
has changed during its life. Perl and Fortran are extreme cases,
but even Lisp has changed a lot. Lisp 1.5 didn't have macros, for
example; these evolved later, after hackers at MIT had spent a
couple years using Lisp to write real programs. [1]So whether or not a language has to be good to be popular, I think
a language has to be popular to be good. And it has to stay popular
to stay good. The state of the art in programming languages doesn't
stand still. And yet the Lisps we have today are still pretty much
what they had at MIT in the mid-1980s, because that's the last time
Lisp had a sufficiently large and demanding user base.Of course, hackers have to know about a language before they can
use it. How are they to hear? From other hackers. But there has to
be some initial group of hackers using the language for others even
to hear about it. I wonder how large this group has to be; how many
users make a critical mass? Off the top of my head, I'd say twenty.
If a language had twenty separate users, meaning twenty users who
decided on their own to use it, I'd consider it to be real.Getting there can't be easy. I would not be surprised if it is
harder to get from zero to twenty than from twenty to a thousand.
The best way to get those initial twenty users is probably to use
a trojan horse: to give people an application they want, which
happens to be written in the new language.2 External FactorsLet's start by acknowledging one external factor that does affect
the popularity of a programming language. To become popular, a
programming language has to be the scripting language of a popular
system. Fortran and Cobol were the scripting languages of early
IBM mainframes. C was the scripting language of Unix, and so, later,
was Perl. Tcl is the scripting language of Tk. Java and Javascript
are intended to be the scripting languages of web browsers.Lisp is not a massively popular language because it is not the
scripting language of a massively popular system. What popularity
it retains dates back to the 1960s and 1970s, when it was the
scripting language of MIT. A lot of the great programmers of the
day were associated with MIT at some point. And in the early 1970s,
before C, MIT's dialect of Lisp, called MacLisp, was one of the
only programming languages a serious hacker would want to use.Today Lisp is the scripting language of two moderately popular
systems, Emacs and Autocad, and for that reason I suspect that most
of the Lisp programming done today is done in Emacs Lisp or AutoLisp.Programming languages don't exist in isolation. To hack is a
transitive verb — hackers are usually hacking something — and in
practice languages are judged relative to whatever they're used to
hack. So if you want to design a popular language, you either have
to supply more than a language, or you have to design your language
to replace the scripting language of some existing system.Common Lisp is unpopular partly because it's an orphan. It did
originally come with a system to hack: the Lisp Machine. But Lisp
Machines (along with parallel computers) were steamrollered by the
increasing power of general purpose processors in the 1980s. Common
Lisp might have remained popular if it had been a good scripting
language for Unix. It is, alas, an atrociously bad one.One way to describe this situation is to say that a language isn't
judged on its own merits. Another view is that a programming language
really isn't a programming language unless it's also the scripting
language of something. This only seems unfair if it comes as a
surprise. I think it's no more unfair than expecting a programming
language to have, say, an implementation. It's just part of what
a programming language is.A programming language does need a good implementation, of course,
and this must be free. Companies will pay for software, but individual
hackers won't, and it's the hackers you need to attract.A language also needs to have a book about it. The book should be
thin, well-written, and full of good examples. K&R is the ideal
here. At the moment I'd almost say that a language has to have a
book published by O'Reilly. That's becoming the test of mattering
to hackers.There should be online documentation as well. In fact, the book
can start as online documentation. But I don't think that physical
books are outmoded yet. Their format is convenient, and the de
facto censorship imposed by publishers is a useful if imperfect
filter. Bookstores are one of the most important places for learning
about new languages.3 BrevityGiven that you can supply the three things any language needs — a
free implementation, a book, and something to hack — how do you
make a language that hackers will like?One thing hackers like is brevity. Hackers are lazy, in the same
way that mathematicians and modernist architects are lazy: they
hate anything extraneous. It would not be far from the truth to
say that a hacker about to write a program decides what language
to use, at least subconsciously, based on the total number of
characters he'll have to type. If this isn't precisely how hackers
think, a language designer would do well to act as if it were.It is a mistake to try to baby the user with long-winded expressions
that are meant to resemble English. Cobol is notorious for this
flaw. A hacker would consider being asked to writeadd x to y giving zinstead ofz = x+yas something between an insult to his intelligence and a sin against
God.It has sometimes been said that Lisp should use first and rest
instead of car and cdr, because it would make programs easier to
read. Maybe for the first couple hours. But a hacker can learn
quickly enough that car means the first element of a list and cdr
means the rest. Using first and rest means 50% more typing. And
they are also different lengths, meaning that the arguments won't
line up when they're called, as car and cdr often are, in successive
lines. I've found that it matters a lot how code lines up on the
page. I can barely read Lisp code when it is set in a variable-width
font, and friends say this is true for other languages too.Brevity is one place where strongly typed languages lose. All other
things being equal, no one wants to begin a program with a bunch
of declarations. Anything that can be implicit, should be.The individual tokens should be short as well. Perl and Common Lisp
occupy opposite poles on this question. Perl programs can be almost
cryptically dense, while the names of built-in Common Lisp operators
are comically long. The designers of Common Lisp probably expected
users to have text editors that would type these long names for
them. But the cost of a long name is not just the cost of typing
it. There is also the cost of reading it, and the cost of the space
it takes up on your screen.4 HackabilityThere is one thing more important than brevity to a hacker: being
able to do what you want. In the history of programming languages
a surprising amount of effort has gone into preventing programmers
from doing things considered to be improper. This is a dangerously
presumptuous plan. How can the language designer know what the
programmer is going to need to do? I think language designers would
do better to consider their target user to be a genius who will
need to do things they never anticipated, rather than a bumbler
who needs to be protected from himself. The bumbler will shoot
himself in the foot anyway. You may save him from referring to
variables in another package, but you can't save him from writing
a badly designed program to solve the wrong problem, and taking
forever to do it.Good programmers often want to do dangerous and unsavory things.
By unsavory I mean things that go behind whatever semantic facade
the language is trying to present: getting hold of the internal
representation of some high-level abstraction, for example. Hackers
like to hack, and hacking means getting inside things and second
guessing the original designer.Let yourself be second guessed. When you make any tool, people use
it in ways you didn't intend, and this is especially true of a
highly articulated tool like a programming language. Many a hacker
will want to tweak your semantic model in a way that you never
imagined. I say, let them; give the programmer access to as much
internal stuff as you can without endangering runtime systems like
the garbage collector.In Common Lisp I have often wanted to iterate through the fields
of a struct — to comb out references to a deleted object, for example,
or find fields that are uninitialized. I know the structs are just
vectors underneath. And yet I can't write a general purpose function
that I can call on any struct. I can only access the fields by
name, because that's what a struct is supposed to mean.A hacker may only want to subvert the intended model of things once
or twice in a big program. But what a difference it makes to be
able to<s> 
The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.
